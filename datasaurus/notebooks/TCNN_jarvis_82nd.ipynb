{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # !pip install -q nnAudio\n",
    "# !pip install -q --upgrade wandb\n",
    "# !pip install -q grad-cam\n",
    "# # !pip install -q ttach\n",
    "# # !pip install efficientnet_pytorch\n",
    "# # !pip install albumentations\n",
    "# !pip install line_profiler\n",
    "# !pip install transformers\n",
    "# !pip install audiomentations\n",
    "# !pip3 install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ipykernel<6\"\n",
    "# !pip install \"jupyterlab<3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import collections\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import sys\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "import h5py\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True) \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import IPython.display\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as torch_functional\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts,\n",
    "                    CosineAnnealingLR, ReduceLROnPlateau,_LRScheduler,CyclicLR)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import audiomentations as A\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, PolarityInversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G2Net-Model/main_82nd_V2_c16/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "\n",
    "    #frequently changed \n",
    "    model_name = 'TCNN'\n",
    "    model_version = \"main_82nd_V2_c16\" \n",
    "    model_module = 'ModelIafossV2'\n",
    "    use_pretrain = False\n",
    "    use_pseudo_label = False\n",
    "    up_thresh = 0.70\n",
    "    down_thresh = 0.15\n",
    "\n",
    "    debug = False\n",
    "    use_checkpoint = False\n",
    "    use_lr_finder = True\n",
    "    use_subset = False \n",
    "    subset_frac = 0.4\n",
    "\n",
    "    #preproc related\n",
    "    #augmentation\n",
    "    vflip = True\n",
    "    \n",
    "    time_shift = False\n",
    "    time_stretch = False\n",
    "\n",
    "    divide_std = False#std changed... tbs\n",
    "    shuffle_channels = False #need normalization first\n",
    "    add_gaussian_noise = False #need normalization first\n",
    "    shuffle01 = False\n",
    "    timemask = False\n",
    "    shift_channel = False    \n",
    "    pitch_shift = False\n",
    "    use_mixup = False\n",
    "    mixup_alpha = 0.1\n",
    "    cropping = False\n",
    "\n",
    "    #logistic\n",
    "    seed = 48\n",
    "    target_size = 1\n",
    "    target_col = 'target'\n",
    "    n_fold = 5\n",
    "#     gdrive = './drive/MyDrive/Kaggle/G2Net/input/'\n",
    "    kaggle_json_path = 'kaggle/kaggle.json'\n",
    "    output_dir = \"G2Net-Model/\"\n",
    "    pseudo_label_folder = \"G2Net-Model/main_35th_GeM_vflip_shuffle01_5fold/\"\n",
    "\n",
    "    #logger\n",
    "    print_num_steps=350\n",
    "    \n",
    "    #training related\n",
    "    train_folds = [0]\n",
    "    epochs = 6\n",
    "    batch_size = 256\n",
    "    \n",
    "    lr= 7e-3#2e-3#8e-3#1e-2#5e-3, 1e-2 # Optimizer  1e-2 channel8, 5e-3 or 2e-3 channel32, 7e-3 channel 16\n",
    "    weight_decay=0 #1e-4  # Optimizer, default value 0.01\n",
    "    gradient_accumulation_steps=1 # Optimizer\n",
    "    scheduler='cosineWithWarmUp' # warm up ratio 0.1 of total steps \n",
    "     \n",
    "    #speedup\n",
    "    num_workers=0\n",
    "    non_blocking=True\n",
    "    amp=True\n",
    "    use_cudnn = True \n",
    "    use_tpu = False\n",
    "    use_ram = True\n",
    "    \n",
    "    #CNN structure\n",
    "    channels = 16\n",
    "    reduction = 4.0\n",
    "    stochastic_final_layer_proba = 0.5\n",
    "\n",
    "# no need to change below\n",
    "Config.model_output_folder = Config.output_dir + Config.model_version + \"/\"\n",
    "if not os.path.exists(Config.output_dir):\n",
    "    os.mkdir(Config.output_dir)\n",
    "if not os.path.exists(Config.model_output_folder):\n",
    "    os.mkdir(Config.model_output_folder)\n",
    "\n",
    "torch.backends.cudnn.benchmark = Config.use_cudnn \n",
    "display(Config.model_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once for Fold 0, save it in RAM and then do experiments multiple times\n",
    "# if Config.train_folds == [0]:\n",
    "#     if Config.use_pseudo_label:\n",
    "#         with open('fold_0_data_PL.npy', 'rb') as f:\n",
    "#             fold_0_data_PL = np.load(f)\n",
    "#     else:\n",
    "#         with open('fold_0_data.npy', 'rb') as f:\n",
    "#             fold_0_data = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "save_object(class2dict(Config), Config.model_output_folder + \"Config.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_2_path(file_id: str, train=True) -> str:\n",
    "    if train:\n",
    "        return \"./output/whiten-train/{}.npy\".format(file_id)\n",
    "    else:\n",
    "        return \"./output/whiten-test/{}.npy\".format(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training_labels.csv')\n",
    "test_df = pd.read_csv('sample_submission.csv')\n",
    "if Config.debug:\n",
    "    Config.epochs = 1\n",
    "    train_df = train_df.sample(n=50000, random_state=Config.seed).reset_index(drop=True)\n",
    "if Config.use_subset:\n",
    "    train_df = train_df.sample(frac=Config.subset_frac, random_state=Config.seed).reset_index(drop=True)\n",
    "train_df['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "test_df['file_path'] = test_df['id'].apply(lambda x :id_2_path(x,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014399681\n",
      "-0.003310739\n",
      "-0.02684033\n",
      "0.01655793\n",
      "-0.02000091\n"
     ]
    }
   ],
   "source": [
    "# checking magnitude of waves\n",
    "num_files = 5\n",
    "input_file_paths = train_df['file_path'].values[:num_files]\n",
    "batch_waves=np.zeros((num_files,3,4096))\n",
    "for i,input_file_path in enumerate(input_file_paths[:num_files]):\n",
    "    file_name = input_file_path.split('/')[-1].split('.npy')[0]\n",
    "    waves = np.load(input_file_path)#.astype(np.float32) # (3, 4096)\n",
    "    # batch_waves[i,:] = np.array([waves.max(),np.abs(waves).max(),np.abs(waves).min()])\n",
    "    whitened_waves = waves#whiten(waves)\n",
    "    print(whitened_waves[2][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold   \n",
       "0     0    0.500125\n",
       "      1    0.499875\n",
       "1     0    0.500125\n",
       "      1    0.499875\n",
       "2     0    0.500125\n",
       "      1    0.499875\n",
       "3     0    0.500125\n",
       "      1    0.499875\n",
       "4     0    0.500125\n",
       "      1    0.499875\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!\n",
    "skf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "splits = skf.split(train_df, train_df[\"target\"])\n",
    "train_df['fold'] = -1\n",
    "for fold, (train_index, valid_index) in enumerate(splits):\n",
    "    train_df.loc[valid_index,\"fold\"] = fold\n",
    "# train_df['fold_PL'] = train_df['fold']\n",
    "\n",
    "train_df.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>file_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000e74ad</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/00000e74ad.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f4945</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/00001f4945.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000661522</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/0000661522.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00007a006a</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/00007a006a.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000a38978</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/0000a38978.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559995</th>\n",
       "      <td>ffff9a5645</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/ffff9a5645.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559996</th>\n",
       "      <td>ffffab0c27</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/ffffab0c27.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559997</th>\n",
       "      <td>ffffcf161a</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/ffffcf161a.npy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559998</th>\n",
       "      <td>ffffd2c403</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/ffffd2c403.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559999</th>\n",
       "      <td>fffff2180b</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/fffff2180b.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  target                             file_path  fold\n",
       "0       00000e74ad       1  ./output/whiten-train/00000e74ad.npy     3\n",
       "1       00001f4945       0  ./output/whiten-train/00001f4945.npy     0\n",
       "2       0000661522       0  ./output/whiten-train/0000661522.npy     4\n",
       "3       00007a006a       0  ./output/whiten-train/00007a006a.npy     0\n",
       "4       0000a38978       1  ./output/whiten-train/0000a38978.npy     4\n",
       "...            ...     ...                                   ...   ...\n",
       "559995  ffff9a5645       1  ./output/whiten-train/ffff9a5645.npy     3\n",
       "559996  ffffab0c27       0  ./output/whiten-train/ffffab0c27.npy     1\n",
       "559997  ffffcf161a       1  ./output/whiten-train/ffffcf161a.npy     2\n",
       "559998  ffffd2c403       0  ./output/whiten-train/ffffd2c403.npy     1\n",
       "559999  fffff2180b       0  ./output/whiten-train/fffff2180b.npy     4\n",
       "\n",
       "[560000 rows x 4 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_list = []\n",
    "\n",
    "if Config.add_gaussian_noise:\n",
    "    transform_list.append(A.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5))\n",
    "if Config.time_shift:\n",
    "    transform_list.append(A.Shift(min_fraction=-512*1.0/4096, max_fraction=-1*1.0/4096, p=0.5,rollover=False))#<0 means shift towards left,  fraction of total sound length\n",
    "# if Config.shift_channel:\n",
    "#     transform_list.append()\n",
    "if Config.pitch_shift:\n",
    "    transform_list.append(A.PitchShift(min_semitones=-1, max_semitones=1, p=0.5))\n",
    "if Config.time_stretch:\n",
    "    transform_list.append(A.TimeStretch(min_rate=0.98, max_rate=1.02,leave_length_unchanged=True, p=0.5))\n",
    "if Config.timemask:\n",
    "    transform_list.append(A.TimeMask(min_band_part=0.0, max_band_part=0.01, fade=False, p=0.5))#try 0.03 next time\n",
    "# if Config.vflip:\n",
    "#     transform_list.append(A.PolarityInversion(p=0.5))\n",
    "\n",
    "train_transform = A.Compose(transform_list)\n",
    "# \n",
    "\n",
    "test_transform = A.Compose([])\n",
    "class DataRetriever(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        # self.ta_augment = ta_Compose([\n",
    "        #     ta_ShuffleChannels(),\n",
    "        # ])#bad coding style \n",
    "        \n",
    "        if Config.use_ram:\n",
    "            start_time =time.time()\n",
    "            array_shape = (len(self.paths),3,4096)\n",
    "            self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "            for i,path in enumerate(self.paths):\n",
    "                waves = np.load(path)\n",
    "                self.data[i,:] = waves\n",
    "                \n",
    "            # saving Fold 0 data for later use\n",
    "#         with open('fold_0_data_PL.npy', 'wb') as f:\n",
    "#             np.save(f, self.data)\n",
    "\n",
    "\n",
    "#           #reading data for fold 0 for fast iteration\n",
    "#         if Config.train_folds == [0]:\n",
    "#             if Config.use_pseudo_label:\n",
    "#                 self.data = fold_0_data_PL\n",
    "#             else:\n",
    "#                 self.data = fold_0_data\n",
    "#         else:\n",
    "#             raise RuntimeError('wrong data!')\n",
    "#             print(time.time()-start_time)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "        if Config.cropping:\n",
    "            waves = waves[:,1792:3840+1]\n",
    "\n",
    "        if Config.divide_std:\n",
    "            waves[0] *= 0.03058\n",
    "            waves[1] *= 0.03058\n",
    "            waves[2] *= 0.03096\n",
    "\n",
    "        if Config.shuffle_channels:#nn.ChannelShuffle\n",
    "            if np.random.random()<0.5:\n",
    "                np.random.shuffle(waves)\n",
    "        if Config.shuffle01:\n",
    "            if np.random.random()<0.5:\n",
    "                waves[[0,1]]=waves[[1,0]]\n",
    "        if Config.vflip:\n",
    "            if np.random.random()<0.5:\n",
    "                waves = -waves\n",
    "              \n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverTest(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        if Config.use_ram:\n",
    "            array_shape = (len(self.paths),3,4096)\n",
    "            self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "            for i,path in enumerate(self.paths):\n",
    "                waves = np.load(path)\n",
    "                self.data[i,:] = waves  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "            \n",
    "        if Config.cropping:\n",
    "            waves = waves[:,1792:3840+1]\n",
    "            \n",
    "        if Config.divide_std:\n",
    "            waves[0] *= 0.03058\n",
    "            waves[1] *= 0.03058\n",
    "            waves[2] *= 0.03096\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverLRFinder(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        # self.ta_augment = ta_Compose([\n",
    "        #     ta_ShuffleChannels(),\n",
    "        # ])#bad coding style        \n",
    "#         start_time =time.time()\n",
    "#         array_shape = (len(self.paths),3,4096)\n",
    "#         self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "#         for i,path in enumerate(self.paths):\n",
    "#             waves = np.load(path)\n",
    "#             self.data[i,:] = waves\n",
    "#         print(time.time()-start_time)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "        \n",
    "        if Config.cropping:\n",
    "            waves = waves[:,1792:3840+1]\n",
    "\n",
    "        if Config.divide_std:\n",
    "            waves[0] *= 0.03058\n",
    "            waves[1] *= 0.03058\n",
    "            waves[2] *= 0.03096\n",
    "\n",
    "        if Config.shuffle_channels:\n",
    "            if np.random.random()<0.5:\n",
    "                np.random.shuffle(waves)\n",
    "        if Config.shuffle01:\n",
    "            if np.random.random()<0.5:\n",
    "                waves[[0,1]]=waves[[1,0]]\n",
    "        if Config.vflip:\n",
    "            if np.random.random()<0.5:\n",
    "                waves = -waves\n",
    "              \n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    '''\n",
    "    Code modified from the 2d code in\n",
    "    https://amaarora.github.io/2020/08/30/gempool.html\n",
    "    '''\n",
    "    def __init__(self, kernel_size=8, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        with torch.cuda.amp.autocast(enabled=False):#to avoid NaN issue for fp16\n",
    "            return torch_functional.avg_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/iafoss/mish-activation\n",
    "import torch.nn.functional as F\n",
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        sigmoid = torch.sigmoid(x)\n",
    "        tanh_sp = torch.tanh(F.softplus(x)) \n",
    "        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MishFunction.apply(x)\n",
    "\n",
    "def to_Mish(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, Mish())\n",
    "        else:\n",
    "            to_Mish(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN_Dilations(nn.Module):\n",
    "    \"\"\"1D convolutional neural network with dilations. Classifier of the gravitaitonal waves\n",
    "    Inspired by the https://arxiv.org/pdf/1904.08693.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_conv = nn.Sequential(nn.Conv1d(3, 256, kernel_size=1), nn.ReLU())\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(256, 256, kernel_size=2, dilation=2 ** i),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(11)\n",
    "            ]\n",
    "        )\n",
    "        self.out_conv = nn.Sequential(nn.Conv1d(256, 1, kernel_size=1), nn.ReLU())\n",
    "        self.fc = nn.Linear(2049, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = self.fc(x)\n",
    "        x.squeeze_(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model1DCNN(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            nn.MaxPool1d(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Model1DCNNGEM(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            GeM(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            GeM(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            GeM(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    \n",
    "\n",
    "#--------------------------------------------------------------------------- V0\n",
    "class ExtractorMaxPool(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.MaxPool1d(kernel_size=maxpool),\n",
    "        )\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes))\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ModelIafoss(nn.Module):\n",
    "    def __init__(self, n=8, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),),\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),)\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,2*n,kernel_size=31,stride=4),\n",
    "            ResBlock(2*n,2*n,kernel_size=31),\n",
    "            ResBlock(2*n,4*n,kernel_size=15,stride=4),\n",
    "            ResBlock(4*n,4*n,kernel_size=15),\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4),\n",
    "            ResBlock(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Flatten(),\n",
    "            nn.Linear(n*8*8,256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "\n",
    "#----------------------------------------------V1    \n",
    "    \n",
    "class AdaptiveConcatPool1d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`\"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "# using GeM\n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "#             nn.MaxPool1d(kernel_size=maxpool),\n",
    "            GeM(kernel_size=maxpool),\n",
    "        )\n",
    "    \n",
    "class ModelIafossV1(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            ResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            ResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            ResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#for SE-----------------------------------------------------------------------------\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, int(channel // reduction), bias=False),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(int(channel // reduction), channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class SEResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1SE(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            SEResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            SEResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            SEResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            SEResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            SEResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            SEResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[1](x[:,1].unsqueeze(1)),\n",
    "            self.ex[2](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#for CBAM-----------------------------------------------------------------------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, silu=True):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(out_planes,eps=1e-5, momentum=0.01, affine=True) #0.01,default momentum 0.1\n",
    "        self.silu = nn.SiLU(inplace=True) if silu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.silu is not None:\n",
    "            x = self.silu(x)\n",
    "        return x\n",
    "    \n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 15\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, silu=True)#silu False\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = torch.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "    \n",
    "class CBAMResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction),\n",
    "            SpatialGate(),\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "    \n",
    "class ModelIafossV1CBAM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            CBAMResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            CBAMResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            CBAMResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            CBAMResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))    \n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class BasicBlockPool(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.MaxPool1d(downsample,ceil_mode=True), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    nn.MaxPool1d(downsample,ceil_mode=True),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                     nn.MaxPool1d(2,ceil_mode=True),  # downsampling \n",
    "#                 )\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "    #             self.shortcut = nn.Sequential(\n",
    "    #                     nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "    #                     nn.BatchNorm1d(out_channels),\n",
    "    #                 )#skip layers in residual_function, can try identity, i.e., nn.Sequential()\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1Pool(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31), #128\n",
    "            BasicBlockPool(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            BasicBlockPool(4*n,4*n,kernel_size=15), #32\n",
    "            BasicBlockPool(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            BasicBlockPool(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class ResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1GeM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAll(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#-----------------------------------------------------------------------------    \n",
    "class AdaptiveConcatPool1dx3(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d`,`AdaptiveMaxPool1d` and 'GeM' \"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "        self.gemp = GeM(kernel_size=8)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x),self.gemp(x)], 1)\n",
    "    \n",
    "class ModelGeMx3(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1dx3(),nn.Flatten(),\n",
    "            nn.Linear(n*8*3,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAllDeep(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "    \n",
    "class StochasticDepthResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=False),p=1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple Pooling\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x\n",
    "    \n",
    "   \n",
    "    \n",
    "class DeepStochastic(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 11\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.conv = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[1]), #128\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[2]), \n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[3]), \n",
    "            StochasticDepthResBlockGeM(3*n,4*n,kernel_size=15,downsample=4,p=self.survival_proba[4]), #128\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[5]), #32\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[7]),\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[9]), #8\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[10]),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#-----------------------------------------------------------------------------\n",
    "class Deeper(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "class Deeper2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=2), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,downsample=2),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),#8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#-------------------------------------------------------------------V2    \n",
    "\n",
    "class ModelIafossV2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,act=act)),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,act=act),#128\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,act=act), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "#-----------------------------------\n",
    "class V2StochasticDepth(nn.Module):#stocnot on ex\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        \n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 10\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5])),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7]),#128\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9]), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Model():\n",
    "    return ModelIafossV2Mish(Config.channels,act=torch.nn.ELU())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    model_name = Config.model_module \n",
    "    if model_name == 'Model1DCNN':\n",
    "        model = Model1DCNN(Config.channels)\n",
    "    elif model_name == 'Model1DCNNGEM':\n",
    "        model = Model1DCNNGEM(Config.channels)\n",
    "    elif model_name == 'ModelIafoss':\n",
    "        model = ModelIafoss(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1':\n",
    "        model = ModelIafossV1(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1SE':\n",
    "        model = ModelIafossV1SE(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1CBAM':\n",
    "        model = ModelIafossV1CBAM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1Pool':\n",
    "        model = ModelIafossV1Pool(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeM':\n",
    "        model = ModelIafossV1GeM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAll':\n",
    "        model = ModelIafossV1GeMAll(Config.channels)\n",
    "    elif model_name == 'ModelGeMx3':\n",
    "        model = ModelGeMx3(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAllDeep':\n",
    "        model = ModelIafossV1GeMAllDeep(Config.channels)\n",
    "    elif model_name == 'DeepStochastic':\n",
    "        model = DeepStochastic(Config.channels)\n",
    "    elif model_name == 'Deeper':\n",
    "        model = Deeper(Config.channels)\n",
    "    elif model_name == 'Deeper2':\n",
    "        model = Deeper2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2':\n",
    "        model = ModelIafossV2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2Mish':\n",
    "        model = ModelIafossV2(Config.channels,act=Mish())\n",
    "    elif model_name == 'ModelIafossV2Elu':\n",
    "        model = ModelIafossV2(Config.channels,act=torch.nn.ELU())\n",
    "    elif model_name == 'V2StochasticDepth':\n",
    "        model = V2StochasticDepth(Config.channels)\n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1548369"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "model = Model()#can possibly call random\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed=Config.seed)    \n",
    "\n",
    "def get_scheduler(optimizer, train_size):\n",
    "    if Config.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=Config.factor, \n",
    "                                      patience=Config.patience, verbose=True, eps=Config.eps)\n",
    "    elif Config.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, \n",
    "                                      T_max=Config.T_max, \n",
    "                                      eta_min=Config.min_lr, last_epoch=-1)\n",
    "    elif Config.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                T_0=Config.T_0, \n",
    "                                                T_mult=1, \n",
    "                                                eta_min=Config.min_lr, \n",
    "                                                last_epoch=-1)\n",
    "    elif Config.scheduler=='CyclicLR':\n",
    "        iter_per_ep = train_size/Config.batch_size\n",
    "        step_size_up = int(iter_per_ep*Config.step_up_epochs)\n",
    "        step_size_down=int(iter_per_ep*Config.step_down_epochs)\n",
    "        scheduler = CyclicLR(optimizer, \n",
    "                             base_lr=Config.base_lr, \n",
    "                             max_lr=Config.max_lr,\n",
    "                             step_size_up=step_size_up,\n",
    "                             step_size_down=step_size_down,\n",
    "                             mode=Config.mode,\n",
    "                             gamma=Config.cycle_decay**(1/(step_size_up+step_size_down)),\n",
    "                             cycle_momentum=False)\n",
    "        \n",
    "    elif Config.scheduler == 'cosineWithWarmUp':\n",
    "        epoch_step = train_size/Config.batch_size\n",
    "        num_warmup_steps = int(0.1 * epoch_step * Config.epochs)\n",
    "        num_training_steps = int(epoch_step * Config.epochs)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=num_warmup_steps, \n",
    "                                                    num_training_steps=num_training_steps)      \n",
    "    return scheduler\n",
    "def mixed_criterion(loss_fn, pred, y_a, y_b, lam):\n",
    "    return lam * loss_fn(pred, y_a) + (1 - lam) * loss_fn(pred, y_b)\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size, requires_grad=False).to(x.device,non_blocking=Config.non_blocking)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-PCIE-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Reserved:    4.3 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "if Config.use_tpu:\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')#for debug, tb see\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "# watch nvidia-smi\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Reserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        torch.save(model.state_dict(), f'{Config.model_output_folder}/init_params.pt')\n",
    "\n",
    "    def range_test(self, loader, end_lr = 10, num_iter = 100, \n",
    "                   smooth_f = 0.05, diverge_th = 5):\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        for step, batch in enumerate(loader):\n",
    "            if step == num_iter:\n",
    "                break\n",
    "            loss = self._train_batch(batch)\n",
    "            lrs.append(lr_scheduler.get_last_lr()[0])\n",
    "            #update lr\n",
    "            lr_scheduler.step()\n",
    "            if step > 0:\n",
    "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "            losses.append(loss)\n",
    "            if loss > diverge_th * best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "        #reset model to initial parameters\n",
    "        model.load_state_dict(torch.load(f'{Config.model_output_folder}/init_params.pt'))\n",
    "        return lrs, losses\n",
    "\n",
    "    def _train_batch(self, batch):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        scaler = GradScaler()\n",
    "        X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "        targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "        \n",
    "        if Config.use_mixup:\n",
    "            (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                X, targets, Config.mixup_alpha\n",
    "            )\n",
    "            with autocast():\n",
    "                outputs = self.model(X_mix).squeeze()\n",
    "                loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "        else:\n",
    "            with autocast():\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "        #loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if Config.use_tpu:\n",
    "            xm.optimizer_step(self.optimizer, barrier=True)  # Note: TPU-specific code! \n",
    "        else:\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "#             self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "                    \n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "def plot_lr_finder(lrs, losses, skip_start = 0, skip_end = 0):\n",
    "    if skip_end == 0:\n",
    "        lrs = lrs[skip_start:]\n",
    "        losses = losses[skip_start:]\n",
    "    else:\n",
    "        lrs = lrs[skip_start:-skip_end]\n",
    "        losses = losses[skip_start:-skip_end]\n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Learning rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True, 'both', 'x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.use_lr_finder:\n",
    "    START_LR = 1e-7\n",
    "    model = Model()\n",
    "    model.to(device,non_blocking=Config.non_blocking)\n",
    "    optimizer = AdamW(model.parameters(), lr=START_LR, weight_decay=Config.weight_decay, amsgrad=False)\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    train_data_retriever = DataRetrieverLRFinder(train_df['file_path'], train_df[\"target\"].values)\n",
    "    train_loader = DataLoader(train_data_retriever,\n",
    "                                batch_size=Config.batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=Config.num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "CPU times: user 53.8 s, sys: 1min 57s, total: 2min 51s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if Config.use_lr_finder:\n",
    "    try:\n",
    "        END_LR = 10\n",
    "        NUM_ITER = 150\n",
    "        lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "        lrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)\n",
    "    except RuntimeError as e:\n",
    "        del model, optimizer, criterion, train_data_retriever, train_loader, lr_finder\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Config.use_lr_finder:\n",
    "    plot_lr_finder(lrs[:-18], losses[:-18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        scheduler,\n",
    "        valid_labels,\n",
    "        best_valid_score,\n",
    "        fold,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.best_valid_score = best_valid_score\n",
    "        self.valid_labels = valid_labels\n",
    "        self.fold = fold\n",
    "\n",
    "    \n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path): \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        global N_EPOCH_EXPLICIT  #tbs later\n",
    "        for n_epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch: ', n_epoch)\n",
    "            N_EPOCH_EXPLICIT = n_epoch\n",
    "            train_loss, train_preds = self.train_epoch(train_loader)\n",
    "            valid_loss, valid_preds = self.valid_epoch(valid_loader)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            if isinstance(self.scheduler, ReduceLROnPlateau):\n",
    "                self.scheduler.step(valid_loss)\n",
    "            valid_score = get_score(self.valid_labels, valid_preds)\n",
    "\n",
    "            numbers = valid_score\n",
    "            filename = Config.model_output_folder+f'score_epoch_{n_epoch}.json'          \n",
    "            with open(filename, 'w') as file_object: \n",
    "                json.dump(numbers, file_object) \n",
    "            \n",
    "\n",
    "            if self.best_valid_score < valid_score:\n",
    "                self.best_valid_score = valid_score\n",
    "                self.save_model(n_epoch, save_path+f'best_model.pth', train_preds, valid_preds)\n",
    "\n",
    "            print('train_loss: ',train_loss)\n",
    "            print('valid_loss: ',valid_loss)\n",
    "            print('valid_score: ',valid_score)\n",
    "            print('best_valid_score: ',self.best_valid_score)\n",
    "            print('time used: ', time.time()-start_time)\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] epoch\": n_epoch+1, \n",
    "                      f\"[fold{self.fold}] avg_train_loss\": train_loss, \n",
    "                      f\"[fold{self.fold}] avg_val_loss\": valid_loss,\n",
    "                      f\"[fold{self.fold}] val_score\": valid_score})        \n",
    "\n",
    "        # fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "        # ax.plot(list(range(epochs)), train_losses, label=\"train_loss\")\n",
    "        # ax.plot(list(range(epochs)), valid_losses, label=\"val_loss\")\n",
    "        # fig.legend()\n",
    "        # plt.show()            \n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        if Config.amp:\n",
    "            scaler = GradScaler()\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        train_loss = 0\n",
    "        # preds = []\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            self.optimizer.zero_grad()\n",
    "            X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "            targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "            \n",
    "            if Config.use_mixup:\n",
    "                (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                    X, targets, Config.mixup_alpha\n",
    "                )\n",
    "                with autocast():\n",
    "                    outputs = self.model(X_mix).squeeze()\n",
    "                    loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                with autocast():\n",
    "                    outputs = self.model(X).squeeze()\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "                \n",
    "            if Config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / Config.gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "          \n",
    "            if (step) % Config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "\n",
    "            if (not isinstance(self.scheduler, ReduceLROnPlateau)):\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # preds.append(outputs.sigmoid().to('cpu').detach().numpy())\n",
    "            loss2 = loss.detach()\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] loss\": loss2,\n",
    "                       f\"[fold{self.fold}] lr\": self.scheduler.get_last_lr()[0]})            \n",
    "\n",
    "            # losses.append(loss2.item())\n",
    "            losses.append(loss2)\n",
    "            train_loss += loss2\n",
    "\n",
    "            if (step) % Config.print_num_steps == 0:\n",
    "                train_loss = train_loss.item() #synch once per print_num_steps instead of once per batch\n",
    "                print(f'[{step}/{len(train_loader)}] ', \n",
    "                      f'avg loss: ',train_loss/step,\n",
    "                      f'inst loss: ', loss2.item())\n",
    "                \n",
    "        # predictions = np.concatenate(preds)\n",
    "\n",
    "#         losses_avg = []\n",
    "#         for i, loss in enumerate(losses):\n",
    "#             if i == 0 :\n",
    "#                 losses_avg.append(loss)\n",
    "#             else:\n",
    "#                 losses_avg.append(losses_avg[-1] * 0.6 + loss * 0.4)\n",
    "#         losses = torch.stack(losses)\n",
    "#         losses_avg = torch.stack(losses_avg)\n",
    "#         fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "#         ax.plot(list(range(step)), losses, label=\"train_loss per step\")\n",
    "#         ax.plot(list(range(step)), losses_avg, label=\"train_loss_avg per step\")\n",
    "#         fig.legend()\n",
    "#         plt.show()            \n",
    "        \n",
    "        return train_loss / step, None#, predictions\n",
    "\n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()      \n",
    "        valid_loss = []\n",
    "        preds = []\n",
    "        for step, batch in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "                targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                if Config.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / Config.gradient_accumulation_steps\n",
    "                valid_loss.append(loss.detach().item())\n",
    "                preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "#                 valid_loss.append(loss.detach())#.item())\n",
    "#                 preds.append(outputs.sigmoid())#.to('cpu').numpy())\n",
    "#         valid_loss = torch.cat(valid_loss).to('cpu').numpy()\n",
    "#         predictions = torch.cat(preds).to('cpu').numpy()\n",
    "        predictions = np.concatenate(preds)\n",
    "        return np.mean(valid_loss), predictions\n",
    "\n",
    "    def save_model(self, n_epoch, save_path, train_preds, valid_preds):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "                'scheduler': self.scheduler.state_dict(),\n",
    "                'train_preds': train_preds,\n",
    "                'valid_preds': valid_preds,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(seed=Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_PL(fold):\n",
    "#     up_thresh = Config.up_thresh\n",
    "#     down_thresh = Config.down_thresh\n",
    "#     pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "#     pseudo_label_df.head()\n",
    "#     pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']#or adding tta\n",
    "#     num_test = pseudo_label_df.shape[0]\n",
    "#     num_yes = (pseudo_label_df[\"target\"] >= up_thresh).sum()\n",
    "#     num_no = (pseudo_label_df[\"target\"] <= down_thresh).sum()\n",
    "#     num_all = num_yes+num_no\n",
    "#     print(\"{:.2%} ratio, {:.2%} 1, {:.2%} 0\".format(num_all/num_test, num_yes/num_test, num_no/num_test))\n",
    "#     print(num_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Config.use_pseudo_label:\n",
    "#     for fold in Config.train_folds:\n",
    "#         check_PL(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-leaky PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_PL(fold,up_thresh,down_thresh,train_df,test_df):\n",
    "    pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "    \n",
    "    #soft labels\n",
    "    pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']\n",
    "    \n",
    "    #harden labels\n",
    "#     test_df_2 = pseudo_label_df[(pseudo_label_df[\"target\"] >= up_thresh) | (pseudo_label_df[\"target\"] <= down_thresh)].copy()\n",
    "#     test_df_2[\"target\"] = (test_df_2[\"target\"] >= up_thresh).astype(int)\n",
    "#     test_df_2 = test_df_2.merge(test_df[[\"id\",\"file_path\"]],on=\"id\",how=\"left\") #no need for this line if already has path\n",
    "    test_df_2 = pseudo_label_df.copy()\n",
    "    test_df_2['fold'] = Config.n_fold\n",
    "    PL_train_df = pd.concat([train_df, test_df_2]).reset_index(drop=True)\n",
    "    PL_train_df.reset_index(inplace=True, drop=True)\n",
    "#         display(train_df_PL.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True)))\n",
    "#         display(train_df_PL.shape)\n",
    "#         display(train_df_PL)\n",
    "    return PL_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_PL(fold,Config.up_thresh,Config.down_thresh,train_df.copy(),test_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_df, use_checkpoint=Config.use_checkpoint):\n",
    "    kf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "    avg_best_valid_score = 0\n",
    "    folds_val_score = []\n",
    "    original_train_df = train_df.copy()#for PL\n",
    "    for fold in range(Config.n_fold): \n",
    "        if Config.use_pseudo_label:\n",
    "            PL_train_df = generate_PL(fold,Config.up_thresh,Config.down_thresh,original_train_df.copy(),test_df)   \n",
    "            train_df = PL_train_df\n",
    "        train_index, valid_index = train_df.query(f\"fold!={fold}\").index, train_df.query(f\"fold=={fold}\").index #fold means fold_valid \n",
    "        print('Fold: ', fold)\n",
    "        if fold not in Config.train_folds:\n",
    "            print(\"skip\")\n",
    "            continue\n",
    "        train_X, valid_X = train_df.loc[train_index], train_df.loc[valid_index]\n",
    "        valid_labels = train_df.loc[valid_index,Config.target_col].values\n",
    "#         fold_indices = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv')#saved fold ids\n",
    "        oof = pd.DataFrame()\n",
    "        oof['id'] = train_df.loc[valid_index,'id']\n",
    "        oof['id'] = valid_X['id'].values.copy()\n",
    "        oof = oof.reset_index()\n",
    "        # assert oof['id'].eq(fold_indices['id']).all()\n",
    "#         if not Config.use_subset:\n",
    "#             assert oof['id'].eq(fold_indices['id']).sum()==112000\n",
    "        oof['target'] = valid_labels\n",
    "        \n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        # continue # uncomment this is to check oof ids\n",
    "\n",
    "        print('training data samples, val data samples: ', len(train_X) ,len(valid_X))\n",
    "        train_data_retriever = DataRetriever(train_X[\"file_path\"].values, train_X[\"target\"].values, transforms=train_transform)#how to run this only once and use for next experiment?\n",
    "        valid_data_retriever = DataRetrieverTest(valid_X[\"file_path\"].values, valid_X[\"target\"].values, transforms=test_transform)        \n",
    "        train_loader = DataLoader(train_data_retriever,\n",
    "                                  batch_size=Config.batch_size, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "        valid_loader = DataLoader(valid_data_retriever, \n",
    "                                  batch_size=Config.batch_size * 2, \n",
    "                                  shuffle=False, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        model = Model()\n",
    "        model.to(device,non_blocking=Config.non_blocking)\n",
    "        optimizer = AdamW(model.parameters(), lr=Config.lr,eps=1e-08, weight_decay=Config.weight_decay, amsgrad=False) #eps to avoid NaN/Inf in training loss\n",
    "        scheduler = get_scheduler(optimizer, len(train_X))\n",
    "        best_valid_score = -np.inf\n",
    "        if use_checkpoint:\n",
    "            print(\"Load Checkpoint, epo\")\n",
    "            checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_valid_score = float(checkpoint['best_valid_score'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        \n",
    "        \n",
    "        criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "        \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model, \n",
    "            device, \n",
    "            optimizer, \n",
    "            criterion,\n",
    "            scheduler,\n",
    "            valid_labels,\n",
    "            best_valid_score,\n",
    "            fold\n",
    "        )\n",
    "\n",
    "        history = trainer.fit(\n",
    "            epochs=Config.epochs, \n",
    "            train_loader=train_loader, \n",
    "            valid_loader=valid_loader,\n",
    "            save_path=f'{Config.model_output_folder}/Fold_{fold}_',\n",
    "        )\n",
    "        folds_val_score.append(trainer.best_valid_score)\n",
    "    wandb.finish()\n",
    "    print('folds score:', folds_val_score)\n",
    "    print(\"Avg: {:.5f}\".format(np.mean(folds_val_score)))\n",
    "    print(\"Std: {:.5f}\".format(np.std(folds_val_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"1b0833b15e81d54fad9cfbbe3d923f57562a6f89\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">main_82nd_V2_c16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kaggle_go/G2Net\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/2bpvbykr\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/2bpvbykr</a><br/>\n",
       "                Run data is saved locally in <code>/home/wandb/run-20210922_161602-2bpvbykr</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_type= \"debug\" if Config.debug else \"train\"\n",
    "# run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=job_type)\n",
    "run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=Config.model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "training data samples, val data samples:  448000 112000\n",
      "Epoch:  0\n",
      "[350/1750]  avg loss:  0.5005491856166294 inst loss:  0.429127961397171\n",
      "[700/1750]  avg loss:  0.4719566563197545 inst loss:  0.42452019453048706\n",
      "[1050/1750]  avg loss:  0.4596232386997768 inst loss:  0.35712188482284546\n",
      "[1400/1750]  avg loss:  0.45126944405691966 inst loss:  0.42017993330955505\n",
      "[1750/1750]  avg loss:  0.44535630580357144 inst loss:  0.4235055446624756\n",
      "train_loss:  0.44535630580357144\n",
      "valid_loss:  0.4733780438769354\n",
      "valid_score:  0.8687695035965635\n",
      "best_valid_score:  0.8687695035965635\n",
      "time used:  259.24374771118164\n",
      "Epoch:  1\n",
      "[350/1750]  avg loss:  0.4185980224609375 inst loss:  0.37873852252960205\n",
      "[700/1750]  avg loss:  0.4174179949079241 inst loss:  0.4525796175003052\n",
      "[1050/1750]  avg loss:  0.41660923549107143 inst loss:  0.4158591032028198\n",
      "[1400/1750]  avg loss:  0.41581529889787944 inst loss:  0.38534724712371826\n",
      "[1750/1750]  avg loss:  0.4140257393973214 inst loss:  0.4306924641132355\n",
      "train_loss:  0.4140257393973214\n",
      "valid_loss:  0.4469576796440229\n",
      "valid_score:  0.8737461391116846\n",
      "best_valid_score:  0.8737461391116846\n",
      "time used:  242.75971722602844\n",
      "Epoch:  2\n",
      "[350/1750]  avg loss:  0.4058694893973214 inst loss:  0.4165775179862976\n",
      "[700/1750]  avg loss:  0.40681884765625 inst loss:  0.4094982445240021\n",
      "[1050/1750]  avg loss:  0.40692740304129466 inst loss:  0.40781649947166443\n",
      "[1400/1750]  avg loss:  0.40664677211216516 inst loss:  0.3830782473087311\n",
      "[1750/1750]  avg loss:  0.40605336216517857 inst loss:  0.40773630142211914\n",
      "train_loss:  0.40605336216517857\n",
      "valid_loss:  0.40315218507971395\n",
      "valid_score:  0.876818090674855\n",
      "best_valid_score:  0.876818090674855\n",
      "time used:  238.18068552017212\n",
      "Epoch:  3\n",
      "[350/1750]  avg loss:  0.40302324567522324 inst loss:  0.39728161692619324\n",
      "[700/1750]  avg loss:  0.40217742919921873 inst loss:  0.42293405532836914\n",
      "[1050/1750]  avg loss:  0.4003300548735119 inst loss:  0.3995090126991272\n",
      "[1400/1750]  avg loss:  0.400135977608817 inst loss:  0.3674126863479614\n",
      "[1750/1750]  avg loss:  0.39942623465401783 inst loss:  0.37586086988449097\n",
      "train_loss:  0.39942623465401783\n",
      "valid_loss:  0.41609019340445463\n",
      "valid_score:  0.8780166232752736\n",
      "best_valid_score:  0.8780166232752736\n",
      "time used:  239.80821323394775\n",
      "Epoch:  4\n",
      "[350/1750]  avg loss:  0.3916867937360491 inst loss:  0.3586061894893646\n",
      "[700/1750]  avg loss:  0.3936273193359375 inst loss:  0.406707763671875\n",
      "[1050/1750]  avg loss:  0.3922978283110119 inst loss:  0.38439708948135376\n",
      "[1400/1750]  avg loss:  0.39214429582868304 inst loss:  0.34735971689224243\n",
      "[1750/1750]  avg loss:  0.39195786830357143 inst loss:  0.43685829639434814\n",
      "train_loss:  0.39195786830357143\n",
      "valid_loss:  0.40334022222044263\n",
      "valid_score:  0.8807404456712777\n",
      "best_valid_score:  0.8807404456712777\n",
      "time used:  237.31297492980957\n",
      "Epoch:  5\n",
      "[350/1750]  avg loss:  0.3846356201171875 inst loss:  0.3471866846084595\n",
      "[700/1750]  avg loss:  0.38360342843191964 inst loss:  0.4138467311859131\n",
      "[1050/1750]  avg loss:  0.38365289597284224 inst loss:  0.34292906522750854\n",
      "[1400/1750]  avg loss:  0.3839048113141741 inst loss:  0.36467817425727844\n",
      "[1750/1750]  avg loss:  0.38372739955357144 inst loss:  0.35779276490211487\n",
      "train_loss:  0.38372739955357144\n",
      "valid_loss:  0.3994249119061858\n",
      "valid_score:  0.8811050300371766\n",
      "best_valid_score:  0.8811050300371766\n",
      "time used:  237.1372458934784\n",
      "Fold:  1\n",
      "skip\n",
      "Fold:  2\n",
      "skip\n",
      "Fold:  3\n",
      "skip\n",
      "Fold:  4\n",
      "skip\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 428<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/wandb/run-20210922_161602-2bpvbykr/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/wandb/run-20210922_161602-2bpvbykr/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold0] avg_train_loss</td><td>0.38373</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.39942</td></tr><tr><td>[fold0] epoch</td><td>6</td></tr><tr><td>[fold0] loss</td><td>0.35779</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] val_score</td><td>0.88111</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold0] avg_train_loss</td><td>â–ˆâ–„â–„â–ƒâ–‚â–</td></tr><tr><td>[fold0] avg_val_loss</td><td>â–ˆâ–…â–â–ƒâ–â–</td></tr><tr><td>[fold0] epoch</td><td>â–â–‚â–„â–…â–‡â–ˆ</td></tr><tr><td>[fold0] loss</td><td>â–‡â–„â–‡â–„â–„â–…â–ƒâ–„â–…â–…â–„â–„â–‚â–‚â–ƒâ–…â–ˆâ–…â–„â–„â–†â–„â–„â–„â–ƒâ–…â–ƒâ–…â–‚â–â–ƒâ–„â–„â–„â–ƒâ–ƒâ–„â–„â–‚â–ƒ</td></tr><tr><td>[fold0] lr</td><td>â–‚â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–</td></tr><tr><td>[fold0] val_score</td><td>â–â–„â–†â–†â–ˆâ–ˆ</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">main_82nd_V2_c16</strong>: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/2bpvbykr\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/2bpvbykr</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folds score: [0.8811050300371766]\n",
      "Avg: 0.88111\n",
      "Std: 0.00000\n",
      "CPU times: user 51min 23s, sys: 1h 57min 29s, total: 2h 48min 53s\n",
      "Wall time: 29min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "#     %lprun -f DataRetriever.__getitem__ -f Trainer.train_epoch -f Trainer.fit -f Trainer.valid_epoch training_loop() \n",
    "    training_loop(train_df,Config.use_checkpoint)\n",
    "except RuntimeError as e:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()   \n",
    "    print(e)# saving oof predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Config.train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# import Ipython\n",
    "# IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarviscloud import jarviscloud\n",
    "jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "successfully saved oof predictions for Fold:  0\n",
      "1\n",
      "successfully saved oof predictions for Fold:  1\n",
      "2\n",
      "successfully saved oof predictions for Fold:  2\n",
      "3\n",
      "successfully saved oof predictions for Fold:  3\n",
      "4\n",
      "successfully saved oof predictions for Fold:  4\n"
     ]
    }
   ],
   "source": [
    "for fold in Config.train_folds:\n",
    "    print(fold)\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    # print(checkpoint['valid_preds'])\n",
    "    try:\n",
    "        # oof = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv') also works, used in replacement of next statement for previously not generated Fold_{fold}_oof_pred.csv\n",
    "        oof = pd.read_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        oof['pred'] = checkpoint['valid_preds']\n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv') \n",
    "        print('successfully saved oof predictions for Fold: ', fold)   \n",
    "    except:\n",
    "        raise RuntimeError('failure in saving predictions for Fold: ', fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTA(Dataset):\n",
    "    def __init__(self, paths, targets, use_vflip=False, shuffle_channels=False, time_shift=False, add_gaussian_noise = False,  time_stretch=False,shuffle01=False ):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.use_vflip = use_vflip\n",
    "        self.shuffle_channels = shuffle_channels\n",
    "        self.time_shift = time_shift\n",
    "        self.gaussian_noise = add_gaussian_noise\n",
    "        self.time_stretch = time_stretch\n",
    "        self.shuffle01 = shuffle01\n",
    "        if time_shift:\n",
    "            self.time_shift = A.Shift(min_fraction=-512*1.0/4096, max_fraction=-1.0/4096, p=1,rollover=False)\n",
    "        if add_gaussian_noise:\n",
    "            self.gaussian_noise = A.AddGaussianNoise(min_amplitude=0.001, max_amplitude= 0.015, p=1)\n",
    "        if time_stretch:\n",
    "            self.time_stretch = A.TimeStretch(min_rate=0.9, max_rate=1.111,leave_length_unchanged=True, p=1)\n",
    "              \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "\n",
    "        if Config.divide_std:\n",
    "            waves[0] *= 0.03058\n",
    "            waves[1] *= 0.03058\n",
    "            waves[2] *= 0.03096\n",
    "\n",
    "        if self.use_vflip:\n",
    "            waves = -waves\n",
    "        if self.shuffle_channels:\n",
    "            np.random.shuffle(waves)\n",
    "        if self.time_shift:\n",
    "            waves = self.time_shift(waves, sample_rate=2048)\n",
    "        if self.gaussian_noise:\n",
    "            waves = self.gaussian_noise(waves, sample_rate=2048)\n",
    "        if self.time_stretch:\n",
    "            waves = self.time_stretch(waves, sample_rate=2048)\n",
    "        if self.shuffle01:\n",
    "            waves[[0,1]] = waves[[1,0]]\n",
    "        \n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device,             \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(loader,model):\n",
    "    preds = []\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        if step % Config.print_num_steps == 0:\n",
    "            print(\"step {}/{}\".format(step, len(loader)))\n",
    "        with torch.no_grad():\n",
    "            X = batch[0].to(device,non_blocking=Config.non_blocking)\n",
    "            outputs = model(X).squeeze()\n",
    "            preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def get_tta_pred(df,model,**transforms):\n",
    "    data_retriever = TTA(df['file_path'].values, df['target'].values, **transforms)\n",
    "    loader = DataLoader(data_retriever, \n",
    "                            batch_size=Config.batch_size * 2, \n",
    "                            shuffle=False, \n",
    "                            num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "    return get_pred(loader,model)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TTA for oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "oof_all = pd.DataFrame()\n",
    "for fold in Config.train_folds:\n",
    "    oof = train_df.query(f\"fold=={fold}\").copy()\n",
    "    oof['preds'] = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')['valid_preds']\n",
    "    oof['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "    # display(oof)    \n",
    "\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    oof[\"tta_vflip\"] = get_tta_pred(oof,model,use_vflip=True)\n",
    "    # oof[\"tta_shift\"] = get_tta_pred(oof,model,time_shift=True)\n",
    "    # oof[\"tta_vflip_shift\"] = get_tta_pred(oof,model,use_vflip=True,time_shift=True)\n",
    "    oof[\"tta_shuffle01\"] = get_tta_pred(oof,model,shuffle01=True)\n",
    "    oof[\"tta_vflip_shuffle01\"] = get_tta_pred(oof,model,use_vflip=True,shuffle01=True)\n",
    "    # oof[\"tta_shift_shuffle01\"] = get_tta_pred(oof,model,time_shift=True,shuffle01=True)\n",
    "    # oof[\"tta_vflip_shift_shuffle01\"] = get_tta_pred(oof,model,use_vflip=True,time_shift=True,shuffle01=True)\n",
    "\n",
    "    oof.to_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\", index=False)\n",
    "    oof_all = pd.concat([oof_all,oof])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.8766354666392064\n",
      "tta_vflip 0.876561745269803\n",
      "tta_shuffle01 0.8764093953304342\n",
      "tta_vflip_shuffle01 0.8766249669510503\n",
      "preds_tta_avg: 0.8774725437897382\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",roc_auc_score(oof_all['target'], oof_all['preds']))\n",
    "\n",
    "for col in oof.columns:\n",
    "    if \"tta\" in col:\n",
    "        print(col,roc_auc_score(oof_all['target'], oof_all[col]))\n",
    "\n",
    "oof_all['avg']=0\n",
    "count = 0\n",
    "for col in oof_all.columns:\n",
    "    if \"tta\" in col or 'preds' in col: \n",
    "        count+=1\n",
    "        oof_all['avg'] += oof_all[col]\n",
    "oof_all['avg'] /= count\n",
    "print(\"preds_tta_avg:\",roc_auc_score(oof_all['target'], oof_all['avg']))\n",
    "\n",
    "oof_all.to_csv(Config.model_output_folder + \"/oof_all.csv\", index=False)\n",
    "oof_all[['id','fold','avg']].rename(columns={'id':'id','fold':'fold','avg':'prediction'}).to_csv(Config.model_output_folder + \"/oof_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TTA for test, to be refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "test_df['target'] = 0  \n",
    "model = Model()\n",
    "test_avg = test_df[['id', 'target']].copy()\n",
    "for fold in Config.train_folds:\n",
    "    test_df2 = test_df.copy()\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "\n",
    "    test_df2['preds'+f'_Fold_{fold}'] = get_tta_pred(test_df2,model)\n",
    "    test_df2[\"tta_vflip\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,use_vflip=True)\n",
    "#     test_df2[\"tta_shift\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,time_shift=True)\n",
    "#     test_df2[\"tta_vflip_shift\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,use_vflip=True,time_shift=True)\n",
    "    test_df2[\"tta_shuffle01\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,shuffle01=True)\n",
    "    test_df2[\"tta_vflip_shuffle01\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,use_vflip=True,shuffle01=True)\n",
    "#     test_df2[\"tta_shift_shuffle01\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,time_shift=True,shuffle01=True)\n",
    "#     test_df2[\"tta_vflip_shift_shuffle01\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,use_vflip=True,time_shift=True,shuffle01=True)\n",
    "    \n",
    "    test_df2.to_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\", index=False)\n",
    "    count = 0\n",
    "    for col in test_df2.columns:\n",
    "        if \"tta\" in col or 'preds' in col: \n",
    "            count+=1\n",
    "            test_avg['target'] += test_df2[col]\n",
    "test_avg['target'] /= count*len(Config.train_folds)\n",
    "test_avg.to_csv(Config.model_output_folder + \"/test_avg.csv\", index=False)\n",
    "\n",
    "#just used vflip here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_Fold_0\n",
      "tta_vflip_Fold_0\n",
      "tta_shuffle01_Fold_0\n",
      "tta_vflip_shuffle01_Fold_0\n",
      "preds_Fold_1\n",
      "tta_vflip_Fold_1\n",
      "tta_shuffle01_Fold_1\n",
      "tta_vflip_shuffle01_Fold_1\n",
      "preds_Fold_2\n",
      "tta_vflip_Fold_2\n",
      "tta_shuffle01_Fold_2\n",
      "tta_vflip_shuffle01_Fold_2\n",
      "preds_Fold_3\n",
      "tta_vflip_Fold_3\n",
      "tta_shuffle01_Fold_3\n",
      "tta_vflip_shuffle01_Fold_3\n",
      "preds_Fold_4\n",
      "tta_vflip_Fold_4\n",
      "tta_shuffle01_Fold_4\n",
      "tta_vflip_shuffle01_Fold_4\n"
     ]
    }
   ],
   "source": [
    "# test_df['target'] = 0  \n",
    "# test_avg = test_df[['id', 'target']].copy()\n",
    "# for fold in [0,1,2,3,4]:\n",
    "#     test_df2= pd.read_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\")\n",
    "#     for col in test_df2.columns:\n",
    "#         if \"tta\" in col or 'pred' in col: \n",
    "#             print(col)\n",
    "#             test_avg['target'] += test_df2[col]\n",
    "# test_avg['target'] /= 20\n",
    "# test_avg.to_csv(Config.model_output_folder + \"/test_avg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg[['id', 'target']].to_csv(\"./submission.csv\", index=False)\n",
    "\n",
    "test_avg[['id', 'target']].to_csv(Config.model_output_folder + \"/submission.csv\", index=False)\n",
    "\n",
    "!mkdir -p ~/.kaggle/ && cp $Config.kaggle_json_path ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c g2net-gravitational-wave-detection -f ./submission.csv -m $Config.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('kaggle': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0324064526588904db53d8c1754501a1e17277e16e25f64624bf6abfe73e224f9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "278px",
    "width": "193.6px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
