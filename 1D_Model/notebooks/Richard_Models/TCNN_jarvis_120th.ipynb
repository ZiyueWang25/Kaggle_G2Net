{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd22902b-841f-41c9-a66f-77395174ae4c",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bb6b65-2c6f-453d-8b6f-c6bf386f446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # !pip install -q nnAudio\n",
    "# !pip install -q --upgrade wandb\n",
    "# !pip install -q grad-cam\n",
    "# # !pip install -q ttach\n",
    "# # !pip install efficientnet_pytorch\n",
    "# # !pip install albumentations\n",
    "# !pip install line_profiler\n",
    "# !pip install transformers\n",
    "# !pip install audiomentations\n",
    "# !pip3 install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2025db-3b0f-43ab-a216-e9058852e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ipykernel<6\"\n",
    "# !pip install \"jupyterlab<3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171b030f-1a09-489c-8b3a-c574d306ade7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import collections\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import sys\n",
    "import json\n",
    "import wandb\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "from glob import glob\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True) \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import IPython.display\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as torch_functional\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts,\n",
    "                    CosineAnnealingLR, ReduceLROnPlateau,_LRScheduler,CyclicLR)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import audiomentations as A\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, PolarityInversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd075088-f395-437c-a176-da7bf90a4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3e2de-86a7-4a6f-ad50-ef450f241250",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c74b277a-2cf0-4edc-8d36-75d33f89d081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G2Net-Model/120th_V2_PL_6ep_1em3lr_32ch_vf_s01/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "\n",
    "    #frequently changed \n",
    "    model_name = 'TCNN'\n",
    "    model_version = \"120th_V2_PL_6ep_1em3lr_32ch_vf_s01\" \n",
    "    model_module = 'ModelIafossV2'#V2StochasticDepth,ModelIafossV2\n",
    "    use_pretrain = False\n",
    "    use_pseudo_label = True\n",
    "    up_thresh = 0.70\n",
    "    down_thresh = 0.15\n",
    "\n",
    "    debug = False\n",
    "    use_checkpoint = False\n",
    "    use_lr_finder = True\n",
    "    use_subset = False \n",
    "    subset_frac = 0.4\n",
    "\n",
    "    #preproc related\n",
    "    #augmentation\n",
    "    #proba for conservative, weight for aggressive\n",
    "    \n",
    "    #conservative\n",
    "    conservative_aug = []#'vflip','add_gaussian_noise',\n",
    "    #aggressive, OneOf \n",
    "    aggressive_aug_proba = 0.75\n",
    "    aggressive_aug = ['vflip','add_gaussian_noise','shuffle01','timemask','time_shift',]     #'reduce_SNR'\n",
    "    \n",
    "    \n",
    "    vflip = True\n",
    "    vflip_proba = 0.5\n",
    "    vflip_weight = 1.0 \n",
    "    add_gaussian_noise = False \n",
    "    add_gaussian_noise_proba = 0.5 \n",
    "    add_gaussian_noise_weight = 1.0    \n",
    "    timemask = False\n",
    "    timemask_proba = 0.35\n",
    "    timemask_weight = 1.0\n",
    "    shuffle01 = True\n",
    "    shuffle01_proba = 0.35\n",
    "    shuffle01_weight = 1.0\n",
    "    time_shift = False\n",
    "    time_shift_left = 96\n",
    "    time_shift_right = 96\n",
    "    time_shift_proba = 0.35\n",
    "    time_shift_weight = 0.5\n",
    "    \n",
    "    shift_channel = False\n",
    "    shift_channel_left = 16\n",
    "    shift_channel_right = 16\n",
    "    shift_channel_proba = 0.5\n",
    "    shift_channel_weight = 1.0\n",
    "    shift_two_channels = False #tba\n",
    "    shift_two_channels_proba = 0.5\n",
    "    shift_two_channels_weight= 1.0\n",
    "    reduce_SNR = False\n",
    "    reduce_SNR_ratio = 0.9998\n",
    "    reduce_SNR_proba = 0.5\n",
    "    reduce_SNR_weight = 1.0\n",
    "\n",
    "    time_stretch = False\n",
    "    divide_std = False \n",
    "    shuffle_channels = False    \n",
    "    pitch_shift = False\n",
    "    use_mixup = False\n",
    "    mixup_alpha = 0.1\n",
    "    cropping = False\n",
    "    \n",
    "    #logistic\n",
    "    seed = 48\n",
    "    target_size = 1\n",
    "    target_col = 'target'\n",
    "    n_fold = 5\n",
    "#     gdrive = './drive/MyDrive/Kaggle/G2Net/input/'\n",
    "    kaggle_json_path = 'kaggle/kaggle.json'\n",
    "    output_dir = \"G2Net-Model/\"\n",
    "    pseudo_label_folder = \"G2Net-Model/main_112th_V2SD_PL_6ep_5Fold/\"#main_35th_GeM_vflip_shuffle01_5fold,#main_112th_V2SD_PL_6ep_5Fold\n",
    "\n",
    "    #logger\n",
    "    print_num_steps=350\n",
    "    \n",
    "    #training related\n",
    "    train_folds = [0,1,2,3,4]\n",
    "    epochs = 6\n",
    "    batch_size = 64\n",
    "    \n",
    "    lr=  1e-3 #2e-3#8e-3#1e-2#5e-3, 1e-2 # Optimizer  1e-2 channel8, 5e-3 or 2e-3 channel32, 7e-3 channel 16\n",
    "    weight_decay=0 #1e-4  # Optimizer, default value 0.01\n",
    "    gradient_accumulation_steps=1 # Optimizer\n",
    "    scheduler='cosineWithWarmUp' # warm up ratio 0.1 of total steps \n",
    "     \n",
    "    #speedup\n",
    "    num_workers=7\n",
    "    non_blocking=False\n",
    "    amp=True\n",
    "    use_cudnn = True \n",
    "    use_tpu = False\n",
    "    use_ram = False\n",
    "    continuous_exp = False\n",
    "    \n",
    "    #CNN structure\n",
    "    channels = 32\n",
    "    reduction = 4.0\n",
    "    stochastic_final_layer_proba = 0.8\n",
    "\n",
    "# no need to change below\n",
    "Config.model_output_folder = Config.output_dir + Config.model_version + \"/\"\n",
    "if not os.path.exists(Config.output_dir):\n",
    "    os.mkdir(Config.output_dir)\n",
    "if not os.path.exists(Config.model_output_folder):\n",
    "    os.mkdir(Config.model_output_folder)\n",
    "\n",
    "torch.backends.cudnn.benchmark = Config.use_cudnn \n",
    "display(Config.model_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fd5ddd-01f0-4e0c-a483-fe055e23bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run once for Fold 0, save it in RAM and then do experiments multiple times       \n",
    "# if Config.continuous_exp and Config.train_folds == [0]:\n",
    "#     start_time =time.time()  \n",
    "#     if Config.use_pseudo_label:\n",
    "#         with open('fold_0_data_PL.npy', 'rb') as f:\n",
    "#             fold_0_data_PL = np.load(f)\n",
    "#     else:\n",
    "#         with open('fold_0_data.npy', 'rb') as f:\n",
    "#             fold_0_data = np.load(f)\n",
    "#     print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d7181-da0e-4f49-b7dc-d1af56fa584c",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b1bd21-a513-4b67-9181-01e1468b1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "save_object(class2dict(Config), Config.model_output_folder + \"Config.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f52f10-a321-463e-be55-6f1267407999",
   "metadata": {},
   "source": [
    "# Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba86f1b-6607-4b9d-a251-f79d9cb7b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_2_path(file_id: str, train=True) -> str:\n",
    "    if train:\n",
    "        return \"./output/whiten-train-w0/{}.npy\".format(file_id)\n",
    "    else:\n",
    "        return \"./output/whiten-test-w0/{}.npy\".format(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a20a41-a049-4ce9-aaf7-61856e1d1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training_labels.csv')\n",
    "test_df = pd.read_csv('sample_submission.csv')\n",
    "if Config.debug:\n",
    "    Config.epochs = 1\n",
    "    train_df = train_df.sample(n=50000, random_state=Config.seed).reset_index(drop=True)\n",
    "if Config.use_subset:\n",
    "    train_df = train_df.sample(frac=Config.subset_frac, random_state=Config.seed).reset_index(drop=True)\n",
    "train_df['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "test_df['file_path'] = test_df['id'].apply(lambda x :id_2_path(x,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adcdac68-2ef7-46b7-91b4-bd52e2585fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.013057893\n",
      "0.01869043\n",
      "0.005618966\n",
      "-0.01463365\n",
      "0.010516807\n"
     ]
    }
   ],
   "source": [
    "# checking magnitude of waves\n",
    "num_files = 5\n",
    "input_file_paths = train_df['file_path'].values[:num_files]\n",
    "batch_waves=np.zeros((num_files,3,4096))\n",
    "for i,input_file_path in enumerate(input_file_paths[:num_files]):\n",
    "    file_name = input_file_path.split('/')[-1].split('.npy')[0]\n",
    "    waves = np.load(input_file_path)#.astype(np.float32) # (3, 4096)\n",
    "#     batch_waves[i,:] = np.array([waves.max(axis=1),np.abs(waves).max(axis=1),np.abs(waves).min(axis=1)])\n",
    "    whitened_waves = waves#whiten(waves)\n",
    "    print(whitened_waves[2][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e6d52c-c6fe-4ef7-bae2-e5dbfcffed6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold   \n",
       "0     0    0.500125\n",
       "      1    0.499875\n",
       "1     0    0.500125\n",
       "      1    0.499875\n",
       "2     0    0.500125\n",
       "      1    0.499875\n",
       "3     0    0.500125\n",
       "      1    0.499875\n",
       "4     0    0.500125\n",
       "      1    0.499875\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!\n",
    "skf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "splits = skf.split(train_df, train_df[\"target\"])\n",
    "train_df['fold'] = -1\n",
    "for fold, (train_index, valid_index) in enumerate(splits):\n",
    "    train_df.loc[valid_index,\"fold\"] = fold\n",
    "# train_df['fold_PL'] = train_df['fold']\n",
    "\n",
    "train_df.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7621283-4686-41fc-a66f-c99b7bfa6143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>file_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000e74ad</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/00000e74ad.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f4945</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/00001f4945.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000661522</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/0000661522.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00007a006a</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/00007a006a.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000a38978</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/0000a38978.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559995</th>\n",
       "      <td>ffff9a5645</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/ffff9a5645.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559996</th>\n",
       "      <td>ffffab0c27</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/ffffab0c27.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559997</th>\n",
       "      <td>ffffcf161a</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/ffffcf161a.npy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559998</th>\n",
       "      <td>ffffd2c403</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/ffffd2c403.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559999</th>\n",
       "      <td>fffff2180b</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/fffff2180b.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  target                                file_path  fold\n",
       "0       00000e74ad       1  ./output/whiten-train-w0/00000e74ad.npy     3\n",
       "1       00001f4945       0  ./output/whiten-train-w0/00001f4945.npy     0\n",
       "2       0000661522       0  ./output/whiten-train-w0/0000661522.npy     4\n",
       "3       00007a006a       0  ./output/whiten-train-w0/00007a006a.npy     0\n",
       "4       0000a38978       1  ./output/whiten-train-w0/0000a38978.npy     4\n",
       "...            ...     ...                                      ...   ...\n",
       "559995  ffff9a5645       1  ./output/whiten-train-w0/ffff9a5645.npy     3\n",
       "559996  ffffab0c27       0  ./output/whiten-train-w0/ffffab0c27.npy     1\n",
       "559997  ffffcf161a       1  ./output/whiten-train-w0/ffffcf161a.npy     2\n",
       "559998  ffffd2c403       0  ./output/whiten-train-w0/ffffd2c403.npy     1\n",
       "559999  fffff2180b       0  ./output/whiten-train-w0/fffff2180b.npy     4\n",
       "\n",
       "[560000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1750c4-a631-4586-9f0a-400fbbae54a6",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b11e6-fe2c-4dcb-80c5-6f6e5acf3903",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446a1dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservative transforms:  []\n",
      "aggressive transforms:  ['vflip', 'shuffle01']\n"
     ]
    }
   ],
   "source": [
    "conserv_transform_list = []\n",
    "aggressive_transform_list = []\n",
    "conserv_transform_list_strings = []\n",
    "aggressive_transform_list_strings = []\n",
    "\n",
    "#-------------------------vflip\n",
    "if Config.vflip:\n",
    "#     trans = lambda x:-x\n",
    "    def vflip_func(x,sample_rate=2048):\n",
    "        return -x\n",
    "    def vflip_func_random(x,sample_rate=2048):\n",
    "        if np.random.random()<Config.vflip_proba:\n",
    "            return -x\n",
    "        else:\n",
    "            return x\n",
    "    if 'vflip' in Config.aggressive_aug:\n",
    "        aggressive_transform_list.append(vflip_func)\n",
    "        aggressive_transform_list_strings.append('vflip')\n",
    "    else:\n",
    "        conserv_transform_list.append(vflip_func_random)\n",
    "        conserv_transform_list_strings.append('vflip')\n",
    "#----------------------add_gaussian_noise        \n",
    "if Config.add_gaussian_noise:\n",
    "    \n",
    "    if 'add_gaussian_noise' in Config.aggressive_aug:\n",
    "        trans = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude=0.015*0.015, p=1) #tbs #0.015 is the estimated std\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('add_gaussian_noise')\n",
    "    else:\n",
    "        trans = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude=0.015*0.015, p=Config.add_gaussian_noise_proba) #tbs #0.015 is the estimated std\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('add_gaussian_noise')\n",
    "\n",
    "#--------------------------timemask\n",
    "if Config.timemask:\n",
    "    \n",
    "    if 'timemask' in Config.aggressive_aug:\n",
    "        trans = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=1)\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('timemask')\n",
    "    else:\n",
    "        trans = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=Config.timemask_proba)\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('timemask')\n",
    "\n",
    "#--------------------------shuffle01        \n",
    "def shuffle01_func(x,sample_rate=2048):\n",
    "    return x[[1,0,2]]\n",
    "def shuffle01_func_random(x,sample_rate=2048):\n",
    "    if np.random.random()<Config.shuffle01_proba: \n",
    "        return x[[1,0,2]]\n",
    "    else:\n",
    "        return x\n",
    "if Config.shuffle01:\n",
    "#     trans = lambda x:x[[1,0,2]]\n",
    "\n",
    "    if 'shuffle01' in Config.aggressive_aug:\n",
    "        aggressive_transform_list.append(shuffle01_func)\n",
    "        aggressive_transform_list_strings.append('shuffle01')\n",
    "    else:\n",
    "        conserv_transform_list.append(shuffle01_func_random)\n",
    "        conserv_transform_list_strings.append('shuffle01')\n",
    "#---------------------------time_shift\n",
    "if Config.time_shift:\n",
    "    if 'time_shift' in Config.aggressive_aug:\n",
    "        trans = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096,\n",
    "                        max_fraction=Config.time_shift_right*1.0/4096, \n",
    "                        p=1,rollover=False)#<0 means shift towards left,  fraction of total sound length\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('time_shift')\n",
    "    else:\n",
    "        trans = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096,\n",
    "                                max_fraction=Config.time_shift_right*1.0/4096, \n",
    "                                p=Config.time_shift_proba,rollover=False)\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('time_shift')\n",
    "\n",
    "#-----------------shift_channel        \n",
    "def shift_channel_func(x,sample_rate=2048):\n",
    "    channel = np.random.choice(3)\n",
    "    trans = A.Shift(min_fraction=-Config.shift_channel_left*1.0/4096,\n",
    "                max_fraction=Config.shift_channel_right*1.0/4096, \n",
    "                p=1,rollover=False)\n",
    "    x[channel] = trans(x[channel],sample_rate=2048)\n",
    "    return x\n",
    "def shift_channel_func_random(x,sample_rate=2048):\n",
    "    channel = np.random.choice(3)\n",
    "    trans = A.Shift(min_fraction=-Config.shift_channel_left*1.0/4096,\n",
    "                max_fraction=Config.shift_channel_right*1.0/4096, \n",
    "                p=Config.shift_channel_proba,rollover=False)\n",
    "    x[channel] = trans(x[channel],sample_rate=2048)\n",
    "    return x\n",
    "if Config.shift_channel:\n",
    "    if 'shift_channel' in Config.aggressive_aug:\n",
    "        \n",
    "        aggressive_transform_list.append(shift_channel_func)\n",
    "        aggressive_transform_list_strings.append('shift_channel')\n",
    "    else:\n",
    "        \n",
    "        conserv_transform_list.append(shift_channel_func_random)\n",
    "        conserv_transform_list_strings.append('shift_channel')\n",
    "#-----------------reduce_SNR        \n",
    "def reduce_SNR_func(x,sample_rate=2048):\n",
    "    x = x * Config.reduce_SNR_ratio\n",
    "    trans = A.AddGaussianNoise(min_amplitude=multiplier, max_amplitude=multiplier, p=1)\n",
    "    x = trans(x,sample_rate=2048)\n",
    "    return x \n",
    "def reduce_SNR_func_random(x,sample_rate=2048):\n",
    "    if np.random.random() < Config.reduce_SNR_proba:\n",
    "        x = x * Config.reduce_SNR_ratio\n",
    "        trans = A.AddGaussianNoise(min_amplitude=multiplier, max_amplitude=multiplier, p=1)\n",
    "        x = trans(x,sample_rate=2048)\n",
    "    return x\n",
    "if Config.reduce_SNR:\n",
    "    multiplier = math.sqrt(1-Config.reduce_SNR_ratio**2)\n",
    "    if 'reduce_SNR' in Config.aggressive_aug:\n",
    "\n",
    "        aggressive_transform_list.append(reduce_SNR_func)\n",
    "        aggressive_transform_list_strings.append('reduce_SNR')\n",
    "    else:\n",
    "\n",
    "        conserv_transform_list.append(reduce_SNR_func_random)\n",
    "        conserv_transform_list_strings.append('reduce_SNR')\n",
    "        \n",
    "# if Config.time_stretch:\n",
    "#     trans = A.TimeStretch(min_rate=0.98, max_rate=1.02,leave_length_unchanged=True, p=0.5)\n",
    "#     if 'time_stretch' in aggressive_aug:\n",
    "#         aggressive_transform_list.append(trans)\n",
    "#         aggressive_transform_list_strings.append('time_stretch')\n",
    "#     else:\n",
    "#         conserv_transform_list.append(trans)\n",
    "#         conserv_transform_list_strings.append('time_stretch')\n",
    "# if Config.pitch_shift:\n",
    "#     trans = A.PitchShift(min_semitones=-1, max_semitones=1, p=0.5)\n",
    "#     if 'pitch_shift' in aggressive_aug:\n",
    "#         aggressive_transform_list.append(trans)\n",
    "#         aggressive_transform_list_strings.append('pitch_shift')\n",
    "#     else:\n",
    "#         conserv_transform_list.append(trans)\n",
    "#         conserv_transform_list_strings.append('pitch_shift')\n",
    "# if Config.shift_channel:\n",
    "#     pass\n",
    "\n",
    "print('conservative transforms: ',conserv_transform_list_strings)\n",
    "print('aggressive transforms: ',aggressive_transform_list_strings)\n",
    "train_transform = conserv_transform_list#A.Compose(conserv_transform_list)#,OneOf(aggressive_transform_list,p=0.5)) # no OneOf in audiomentation\n",
    "# \n",
    "\n",
    "test_transform = None #A.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a66e416-c7ac-4526-b32a-96d6aa50190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [getattr(Config(), f'{agg}_weight') for agg in aggressive_transform_list_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6f8d25-feda-4576-9080-b2e40228ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRetriever(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "\n",
    "\n",
    "        #reading data for fold 0 for fast iteration\n",
    "        if Config.continuous_exp and Config.train_folds == [0]:\n",
    "            if Config.use_pseudo_label:\n",
    "                self.data = fold_0_data_PL\n",
    "            else:\n",
    "                self.data = fold_0_data\n",
    "        else:\n",
    "            if Config.use_ram:\n",
    "                start_time =time.time()\n",
    "                array_shape = (len(self.paths),3,4096)\n",
    "                self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "                for i,path in enumerate(self.paths):\n",
    "                    waves = np.load(path)\n",
    "                    self.data[i,:] = waves            \n",
    "                print(time.time()-start_time)\n",
    "\n",
    "                \n",
    "            # saving Fold 0 data for later use\n",
    "#         with open('fold_0_data_PL.npy', 'wb') as f:\n",
    "#             np.save(f, self.data)\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015 #causing NaN?\n",
    "\n",
    "#         if Config.shuffle_channels:#nn.ChannelShuffle\n",
    "#             if np.random.random()<0.5:\n",
    "#                 np.random.shuffle(waves)\n",
    "                \n",
    "#         if Config.vflip:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves = -waves\n",
    "            \n",
    "        if self.transforms is not None:\n",
    "            for i,_ in enumerate(self.transforms):\n",
    "                transform = conserv_transform_list[i]\n",
    "                waves= transform(waves,sample_rate=2048)\n",
    "            \n",
    "        if aggressive_transform_list_strings:\n",
    "            if np.random.random()<Config.aggressive_aug_proba:\n",
    "                n = len(aggressive_transform_list_strings)\n",
    "                probas = np.array([getattr(Config(), f'{agg}_weight') for agg in aggressive_transform_list_strings])\n",
    "                probas /= probas.sum()\n",
    "                trans_idx = np.random.choice(n,p=probas)\n",
    "                trans = aggressive_transform_list[trans_idx]\n",
    "                waves = trans(waves,sample_rate=2048)\n",
    "\n",
    "\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverTest(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        if Config.use_ram:\n",
    "            array_shape = (len(self.paths),3,4096)\n",
    "            self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "            for i,path in enumerate(self.paths):\n",
    "                waves = np.load(path)\n",
    "                self.data[i,:] = waves  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "            \n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "            \n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverLRFinder(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms     \n",
    "#         start_time =time.time()\n",
    "#         array_shape = (len(self.paths),3,4096)\n",
    "#         self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "#         for i,path in enumerate(self.paths):\n",
    "#             waves = np.load(path)\n",
    "#             self.data[i,:] = waves\n",
    "#         print(time.time()-start_time)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "        \n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "#         if Config.shuffle_channels:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 np.random.shuffle(waves)\n",
    "#         if Config.shuffle01:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves[[0,1]]=waves[[1,0]]\n",
    "#         if Config.vflip:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves = -waves\n",
    "              \n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b86ab7a-b88e-4a0f-9cb7-3a023ae12408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(aggressive_transform_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6f55460-98fb-47e8-b4b6-3d898ff751da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice(5,p=[0.1, 0, 0.3, 0.6, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d3d1abe-3f31-4b01-bf9a-15b300461b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    '''\n",
    "    Code modified from the 2d code in\n",
    "    https://amaarora.github.io/2020/08/30/gempool.html\n",
    "    '''\n",
    "    def __init__(self, kernel_size=8, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        with torch.cuda.amp.autocast(enabled=False):#to avoid NaN issue for fp16\n",
    "            return torch_functional.avg_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45273271-348f-453b-a905-01e846d7bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/iafoss/mish-activation\n",
    "import torch.nn.functional as F\n",
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        sigmoid = torch.sigmoid(x)\n",
    "        tanh_sp = torch.tanh(F.softplus(x)) \n",
    "        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MishFunction.apply(x)\n",
    "\n",
    "def to_Mish(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, Mish())\n",
    "        else:\n",
    "            to_Mish(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd2ba5-d403-4abf-b971-2303a92ea8ea",
   "metadata": {},
   "source": [
    "## neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de5c4e68-068e-451e-aa4c-ac1e28ed5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN_Dilations(nn.Module):\n",
    "    \"\"\"1D convolutional neural network with dilations. Classifier of the gravitaitonal waves\n",
    "    Inspired by the https://arxiv.org/pdf/1904.08693.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_conv = nn.Sequential(nn.Conv1d(3, 256, kernel_size=1), nn.ReLU())\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(256, 256, kernel_size=2, dilation=2 ** i),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(11)\n",
    "            ]\n",
    "        )\n",
    "        self.out_conv = nn.Sequential(nn.Conv1d(256, 1, kernel_size=1), nn.ReLU())\n",
    "        self.fc = nn.Linear(2049, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = self.fc(x)\n",
    "        x.squeeze_(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model1DCNN(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            nn.MaxPool1d(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Model1DCNNGEM(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            GeM(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            GeM(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            GeM(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    \n",
    "\n",
    "#--------------------------------------------------------------------------- V0\n",
    "class ExtractorMaxPool(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.MaxPool1d(kernel_size=maxpool),\n",
    "        )\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes))\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ModelIafoss(nn.Module):\n",
    "    def __init__(self, n=8, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),),\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),)\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,2*n,kernel_size=31,stride=4),\n",
    "            ResBlock(2*n,2*n,kernel_size=31),\n",
    "            ResBlock(2*n,4*n,kernel_size=15,stride=4),\n",
    "            ResBlock(4*n,4*n,kernel_size=15),\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4),\n",
    "            ResBlock(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Flatten(),\n",
    "            nn.Linear(n*8*8,256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "\n",
    "#----------------------------------------------V1    \n",
    "    \n",
    "class AdaptiveConcatPool1d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`\"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "# using GeM\n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "#             nn.MaxPool1d(kernel_size=maxpool),\n",
    "            GeM(kernel_size=maxpool),\n",
    "        )\n",
    "    \n",
    "class ModelIafossV1(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            ResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            ResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            ResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#for SE-----------------------------------------------------------------------------\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, int(channel // reduction), bias=False),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(int(channel // reduction), channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class SEResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1SE(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            SEResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            SEResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            SEResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            SEResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            SEResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            SEResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[1](x[:,1].unsqueeze(1)),\n",
    "            self.ex[2](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#for CBAM-----------------------------------------------------------------------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, silu=True):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(out_planes,eps=1e-5, momentum=0.01, affine=True) #0.01,default momentum 0.1\n",
    "        self.silu = nn.SiLU(inplace=True) if silu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.silu is not None:\n",
    "            x = self.silu(x)\n",
    "        return x\n",
    "    \n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 15\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, silu=True)#silu False\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = torch.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "    \n",
    "class CBAMResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction),\n",
    "            SpatialGate(),\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "    \n",
    "class ModelIafossV1CBAM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            CBAMResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            CBAMResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            CBAMResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            CBAMResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))    \n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class BasicBlockPool(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.MaxPool1d(downsample,ceil_mode=True), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    nn.MaxPool1d(downsample,ceil_mode=True),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                     nn.MaxPool1d(2,ceil_mode=True),  # downsampling \n",
    "#                 )\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "    #             self.shortcut = nn.Sequential(\n",
    "    #                     nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "    #                     nn.BatchNorm1d(out_channels),\n",
    "    #                 )#skip layers in residual_function, can try identity, i.e., nn.Sequential()\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1Pool(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31), #128\n",
    "            BasicBlockPool(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            BasicBlockPool(4*n,4*n,kernel_size=15), #32\n",
    "            BasicBlockPool(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            BasicBlockPool(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class ResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1GeM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAll(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#-----------------------------------------------------------------------------    \n",
    "class AdaptiveConcatPool1dx3(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d`,`AdaptiveMaxPool1d` and 'GeM' \"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "        self.gemp = GeM(kernel_size=8)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x),self.gemp(x)], 1)\n",
    "    \n",
    "class ModelGeMx3(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1dx3(),nn.Flatten(),\n",
    "            nn.Linear(n*8*3,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAllDeep(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "    \n",
    "class StochasticDepthResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=False),p=1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple Pooling\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x\n",
    "    \n",
    "   \n",
    "    \n",
    "class DeepStochastic(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 11\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.conv = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[1]), #128\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[2]), \n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[3]), \n",
    "            StochasticDepthResBlockGeM(3*n,4*n,kernel_size=15,downsample=4,p=self.survival_proba[4]), #128\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[5]), #32\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[7]),\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[9]), #8\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[10]),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#-----------------------------------------------------------------------------\n",
    "class Deeper(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "class Deeper2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=2), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,downsample=2),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),#8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#-------------------------------------------------------------------V2    \n",
    "\n",
    "class ModelIafossV2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,act=act)),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,act=act),#128\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,act=act), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "#-----------------------------------\n",
    "class V2StochasticDepth(nn.Module):#stocnot on ex\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        \n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 10\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5])),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7]),#128\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9]), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e1528ce-06af-4d53-84d9-bbbcaf4991b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    model_name = Config.model_module \n",
    "    if model_name == 'Model1DCNN':\n",
    "        model = Model1DCNN(Config.channels)\n",
    "    elif model_name == 'Model1DCNNGEM':\n",
    "        model = Model1DCNNGEM(Config.channels)\n",
    "    elif model_name == 'ModelIafoss':\n",
    "        model = ModelIafoss(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1':\n",
    "        model = ModelIafossV1(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1SE':\n",
    "        model = ModelIafossV1SE(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1CBAM':\n",
    "        model = ModelIafossV1CBAM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1Pool':\n",
    "        model = ModelIafossV1Pool(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeM':\n",
    "        model = ModelIafossV1GeM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAll':\n",
    "        model = ModelIafossV1GeMAll(Config.channels)\n",
    "    elif model_name == 'ModelGeMx3':\n",
    "        model = ModelGeMx3(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAllDeep':\n",
    "        model = ModelIafossV1GeMAllDeep(Config.channels)\n",
    "    elif model_name == 'DeepStochastic':\n",
    "        model = DeepStochastic(Config.channels)\n",
    "    elif model_name == 'Deeper':\n",
    "        model = Deeper(Config.channels)\n",
    "    elif model_name == 'Deeper2':\n",
    "        model = Deeper2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2':\n",
    "        model = ModelIafossV2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2Mish':\n",
    "        model = ModelIafossV2(Config.channels,act=Mish())\n",
    "    elif model_name == 'ModelIafossV2Elu':\n",
    "        model = ModelIafossV2(Config.channels,act=torch.nn.ELU())\n",
    "    elif model_name == 'V2StochasticDepth':\n",
    "        model = V2StochasticDepth(Config.channels)\n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "    print(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ce418f8-18dc-40c4-ac7b-fe4384f44245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5845905"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "model = Model()#can possibly call random\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c9f2c-fc5f-41ee-afa5-2dfbec2ef82f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b76b69f-3f6d-40dd-92e9-b04be3232814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed=Config.seed)    \n",
    "\n",
    "def get_scheduler(optimizer, train_size):\n",
    "    if Config.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=Config.factor, \n",
    "                                      patience=Config.patience, verbose=True, eps=Config.eps)\n",
    "    elif Config.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, \n",
    "                                      T_max=Config.T_max, \n",
    "                                      eta_min=Config.min_lr, last_epoch=-1)\n",
    "    elif Config.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                T_0=Config.T_0, \n",
    "                                                T_mult=1, \n",
    "                                                eta_min=Config.min_lr, \n",
    "                                                last_epoch=-1)\n",
    "    elif Config.scheduler=='CyclicLR':\n",
    "        iter_per_ep = train_size/Config.batch_size\n",
    "        step_size_up = int(iter_per_ep*Config.step_up_epochs)\n",
    "        step_size_down=int(iter_per_ep*Config.step_down_epochs)\n",
    "        scheduler = CyclicLR(optimizer, \n",
    "                             base_lr=Config.base_lr, \n",
    "                             max_lr=Config.max_lr,\n",
    "                             step_size_up=step_size_up,\n",
    "                             step_size_down=step_size_down,\n",
    "                             mode=Config.mode,\n",
    "                             gamma=Config.cycle_decay**(1/(step_size_up+step_size_down)),\n",
    "                             cycle_momentum=False)\n",
    "        \n",
    "    elif Config.scheduler == 'cosineWithWarmUp':\n",
    "        epoch_step = train_size/Config.batch_size\n",
    "        num_warmup_steps = int(0.1 * epoch_step * Config.epochs)\n",
    "        num_training_steps = int(epoch_step * Config.epochs)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=num_warmup_steps, \n",
    "                                                    num_training_steps=num_training_steps)      \n",
    "    return scheduler\n",
    "def mixed_criterion(loss_fn, pred, y_a, y_b, lam):\n",
    "    return lam * loss_fn(pred, y_a) + (1 - lam) * loss_fn(pred, y_b)\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size, requires_grad=False).to(x.device,non_blocking=Config.non_blocking)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "498ba49a-52a8-4e90-9b51-4fb267a03350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-PCIE-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Reserved:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "if Config.use_tpu:\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')#for debug, tb see\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "# watch nvidia-smi\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Reserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588c2fb-6f2f-4a71-ae80-af4e7facbf40",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1791f02a-cd63-448f-ad07-a216643c196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        torch.save(model.state_dict(), f'{Config.model_output_folder}/init_params.pt')\n",
    "\n",
    "    def range_test(self, loader, end_lr = 10, num_iter = 100, \n",
    "                   smooth_f = 0.05, diverge_th = 5):\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        for step, batch in enumerate(loader):\n",
    "            if step == num_iter:\n",
    "                break\n",
    "            loss = self._train_batch(batch)\n",
    "            lrs.append(lr_scheduler.get_last_lr()[0])\n",
    "            #update lr\n",
    "            lr_scheduler.step()\n",
    "            if step > 0:\n",
    "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "            losses.append(loss)\n",
    "            if loss > diverge_th * best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "        #reset model to initial parameters\n",
    "        model.load_state_dict(torch.load(f'{Config.model_output_folder}/init_params.pt'))\n",
    "        return lrs, losses\n",
    "\n",
    "    def _train_batch(self, batch):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        scaler = GradScaler()\n",
    "        X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "        targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "        \n",
    "        if Config.use_mixup:\n",
    "            (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                X, targets, Config.mixup_alpha\n",
    "            )\n",
    "            with autocast():\n",
    "                outputs = self.model(X_mix).squeeze()\n",
    "                loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "        else:\n",
    "            with autocast():\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "        #loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if Config.use_tpu:\n",
    "            xm.optimizer_step(self.optimizer, barrier=True)  # Note: TPU-specific code! \n",
    "        else:\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "#             self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "                    \n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "def plot_lr_finder(lrs, losses, skip_start = 0, skip_end = 0):\n",
    "    if skip_end == 0:\n",
    "        lrs = lrs[skip_start:]\n",
    "        losses = losses[skip_start:]\n",
    "    else:\n",
    "        lrs = lrs[skip_start:-skip_end]\n",
    "        losses = losses[skip_start:-skip_end]\n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Learning rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True, 'both', 'x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0acca149-21fd-4ee7-9195-6e9c4ab3343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "if Config.use_lr_finder:\n",
    "    START_LR = 1e-7\n",
    "    model = Model()\n",
    "    model.to(device,non_blocking=Config.non_blocking)\n",
    "    optimizer = AdamW(model.parameters(), lr=START_LR, weight_decay=Config.weight_decay, amsgrad=False)\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    train_data_retriever = DataRetrieverLRFinder(train_df['file_path'], train_df[\"target\"].values)\n",
    "    train_loader = DataLoader(train_data_retriever,\n",
    "                                batch_size=Config.batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=Config.num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a155a5-34a6-4f13-bca8-ea8efaade09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13a2e828-f63d-4564-874b-b56924fb62e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "CPU times: user 12.5 s, sys: 2.75 s, total: 15.2 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if Config.use_lr_finder:\n",
    "    try:\n",
    "        END_LR = 10\n",
    "        NUM_ITER = 150\n",
    "        lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "        lrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)\n",
    "    except RuntimeError as e:\n",
    "        del model, optimizer, criterion, train_data_retriever, train_loader, lr_finder\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c0570b0-2648-4761-993f-66b8aa6e5118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Config.use_lr_finder:\n",
    "    plot_lr_finder(lrs[:-18], losses[:-18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfb7f1-6e67-4dbe-9244-2d08a880b108",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70036313-ffa6-4249-a4b8-faadad8bafa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        scheduler,\n",
    "        valid_labels,\n",
    "        best_valid_score,\n",
    "        fold,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.best_valid_score = best_valid_score\n",
    "        self.valid_labels = valid_labels\n",
    "        self.fold = fold\n",
    "\n",
    "    \n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path): \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "#         global N_EPOCH_EXPLICIT  #tbs later\n",
    "        for n_epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch: ', n_epoch)\n",
    "            N_EPOCH_EXPLICIT = n_epoch\n",
    "            train_loss, train_preds = self.train_epoch(train_loader)\n",
    "            valid_loss, valid_preds = self.valid_epoch(valid_loader)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            if isinstance(self.scheduler, ReduceLROnPlateau):\n",
    "                self.scheduler.step(valid_loss)\n",
    "            valid_score = get_score(self.valid_labels, valid_preds)\n",
    "\n",
    "            numbers = valid_score\n",
    "            filename = Config.model_output_folder+f'score_epoch_{n_epoch}.json'          \n",
    "            with open(filename, 'w') as file_object: \n",
    "                json.dump(numbers, file_object) \n",
    "            \n",
    "\n",
    "            if self.best_valid_score < valid_score:\n",
    "                self.best_valid_score = valid_score\n",
    "                self.save_model(n_epoch, save_path+f'best_model.pth', train_preds, valid_preds)\n",
    "\n",
    "            print('train_loss: ',train_loss)\n",
    "            print('valid_loss: ',valid_loss)\n",
    "            print('valid_score: ',valid_score)\n",
    "            print('best_valid_score: ',self.best_valid_score)\n",
    "            print('time used: ', time.time()-start_time)\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] epoch\": n_epoch+1, \n",
    "                      f\"[fold{self.fold}] avg_train_loss\": train_loss, \n",
    "                      f\"[fold{self.fold}] avg_val_loss\": valid_loss,\n",
    "                      f\"[fold{self.fold}] val_score\": valid_score})        \n",
    "\n",
    "        # fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "        # ax.plot(list(range(epochs)), train_losses, label=\"train_loss\")\n",
    "        # ax.plot(list(range(epochs)), valid_losses, label=\"val_loss\")\n",
    "        # fig.legend()\n",
    "        # plt.show()            \n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        if Config.amp:\n",
    "            scaler = GradScaler()\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        train_loss = 0\n",
    "        # preds = []\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            self.optimizer.zero_grad()\n",
    "            X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "            targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "            \n",
    "            if Config.use_mixup:\n",
    "                (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                    X, targets, Config.mixup_alpha\n",
    "                )\n",
    "                with autocast():\n",
    "                    outputs = self.model(X_mix).squeeze()\n",
    "                    loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                with autocast():\n",
    "                    outputs = self.model(X).squeeze()\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "\n",
    "                \n",
    "            if Config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / Config.gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "          \n",
    "            if (step) % Config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "\n",
    "            if (not isinstance(self.scheduler, ReduceLROnPlateau)):\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # preds.append(outputs.sigmoid().to('cpu').detach().numpy())\n",
    "            loss2 = loss.detach()\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] loss\": loss2,\n",
    "                       f\"[fold{self.fold}] lr\": self.scheduler.get_last_lr()[0]})            \n",
    "\n",
    "            # losses.append(loss2.item())\n",
    "            losses.append(loss2)\n",
    "            train_loss += loss2\n",
    "\n",
    "            if (step) % Config.print_num_steps == 0:\n",
    "                train_loss = train_loss.item() #synch once per print_num_steps instead of once per batch\n",
    "                print(f'[{step}/{len(train_loader)}] ', \n",
    "                      f'avg loss: ',train_loss/step,\n",
    "                      f'inst loss: ', loss2.item())\n",
    "                \n",
    "        # predictions = np.concatenate(preds)\n",
    "\n",
    "#         losses_avg = []\n",
    "#         for i, loss in enumerate(losses):\n",
    "#             if i == 0 :\n",
    "#                 losses_avg.append(loss)\n",
    "#             else:\n",
    "#                 losses_avg.append(losses_avg[-1] * 0.6 + loss * 0.4)\n",
    "#         losses = torch.stack(losses)\n",
    "#         losses_avg = torch.stack(losses_avg)\n",
    "#         fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "#         ax.plot(list(range(step)), losses, label=\"train_loss per step\")\n",
    "#         ax.plot(list(range(step)), losses_avg, label=\"train_loss_avg per step\")\n",
    "#         fig.legend()\n",
    "#         plt.show()            \n",
    "        \n",
    "        return train_loss / step, None#, predictions\n",
    "\n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()      \n",
    "        valid_loss = []\n",
    "        preds = []\n",
    "        for step, batch in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "                targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                if Config.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / Config.gradient_accumulation_steps\n",
    "                valid_loss.append(loss.detach().item())\n",
    "                preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "#                 valid_loss.append(loss.detach())#.item())\n",
    "#                 preds.append(outputs.sigmoid())#.to('cpu').numpy())\n",
    "#         valid_loss = torch.cat(valid_loss).to('cpu').numpy()\n",
    "#         predictions = torch.cat(preds).to('cpu').numpy()\n",
    "        predictions = np.concatenate(preds)\n",
    "        return np.mean(valid_loss), predictions\n",
    "\n",
    "    def save_model(self, n_epoch, save_path, train_preds, valid_preds):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "                'scheduler': self.scheduler.state_dict(),\n",
    "                'train_preds': train_preds,\n",
    "                'valid_preds': valid_preds,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0dfd0b-83ea-48f9-aca0-7e84b3346689",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f26270c-7a97-4d8a-80bd-db5ec6ad345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(seed=Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cce08b8-36a9-4fe2-b764-b612927f3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_PL(fold):\n",
    "#     up_thresh = Config.up_thresh\n",
    "#     down_thresh = Config.down_thresh\n",
    "#     pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "#     pseudo_label_df.head()\n",
    "#     pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']#or adding tta\n",
    "#     num_test = pseudo_label_df.shape[0]\n",
    "#     num_yes = (pseudo_label_df[\"target\"] >= up_thresh).sum()\n",
    "#     num_no = (pseudo_label_df[\"target\"] <= down_thresh).sum()\n",
    "#     num_all = num_yes+num_no\n",
    "#     print(\"{:.2%} ratio, {:.2%} 1, {:.2%} 0\".format(num_all/num_test, num_yes/num_test, num_no/num_test))\n",
    "#     print(num_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae6c66a1-0cda-4447-b5c8-017af0d03874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Config.use_pseudo_label:\n",
    "#     for fold in Config.train_folds:\n",
    "#         check_PL(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aed9b-d467-49b5-9611-8a170be6d00a",
   "metadata": {},
   "source": [
    "## non-leaky PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14d354a8-d7e3-4bbb-85ee-f5e6a0126652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_PL(fold,up_thresh,down_thresh,train_df,test_df):\n",
    "    pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "    \n",
    "    #soft labels\n",
    "    pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']\n",
    "    \n",
    "    #harden labels\n",
    "#     test_df_2 = pseudo_label_df[(pseudo_label_df[\"target\"] >= up_thresh) | (pseudo_label_df[\"target\"] <= down_thresh)].copy()\n",
    "#     test_df_2[\"target\"] = (test_df_2[\"target\"] >= up_thresh).astype(int)\n",
    "#     test_df_2 = test_df_2.merge(test_df[[\"id\",\"file_path\"]],on=\"id\",how=\"left\") #no need for this line if already has path\n",
    "    test_df_2 = pseudo_label_df.copy()\n",
    "    test_df_2['fold'] = Config.n_fold\n",
    "    PL_train_df = pd.concat([train_df, test_df_2]).reset_index(drop=True)\n",
    "    PL_train_df.reset_index(inplace=True, drop=True)\n",
    "#         display(train_df_PL.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True)))\n",
    "#         display(train_df_PL.shape)\n",
    "#         display(train_df_PL)\n",
    "    return PL_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdaf0da2-d079-4982-bb12-8ff2ea2751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_PL(fold,Config.up_thresh,Config.down_thresh,train_df.copy(),test_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1aff774-130a-4ba1-9132-5a7a60553992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_df, use_checkpoint=Config.use_checkpoint):\n",
    "    kf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "    avg_best_valid_score = 0\n",
    "    folds_val_score = []\n",
    "    original_train_df = train_df.copy()#for PL\n",
    "    for fold in range(Config.n_fold): \n",
    "        if Config.use_pseudo_label:\n",
    "            PL_train_df = generate_PL(fold,Config.up_thresh,Config.down_thresh,original_train_df.copy(),test_df)   \n",
    "            train_df = PL_train_df\n",
    "        train_index, valid_index = train_df.query(f\"fold!={fold}\").index, train_df.query(f\"fold=={fold}\").index #fold means fold_valid \n",
    "        print('Fold: ', fold)\n",
    "        if fold not in Config.train_folds:\n",
    "            print(\"skip\")\n",
    "            continue\n",
    "        train_X, valid_X = train_df.loc[train_index], train_df.loc[valid_index]\n",
    "        valid_labels = train_df.loc[valid_index,Config.target_col].values\n",
    "#         fold_indices = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv')#saved fold ids\n",
    "        oof = pd.DataFrame()\n",
    "        oof['id'] = train_df.loc[valid_index,'id']\n",
    "        oof['id'] = valid_X['id'].values.copy()\n",
    "        oof = oof.reset_index()\n",
    "        # assert oof['id'].eq(fold_indices['id']).all()\n",
    "#         if not Config.use_subset:\n",
    "#             assert oof['id'].eq(fold_indices['id']).sum()==112000\n",
    "        oof['target'] = valid_labels\n",
    "        \n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        # continue # uncomment this is to check oof ids\n",
    "\n",
    "        print('training data samples, val data samples: ', len(train_X) ,len(valid_X))\n",
    "        train_data_retriever = DataRetriever(train_X[\"file_path\"].values, train_X[\"target\"].values, transforms=train_transform)#how to run this only once and use for next experiment?\n",
    "        valid_data_retriever = DataRetrieverTest(valid_X[\"file_path\"].values, valid_X[\"target\"].values, transforms=test_transform)        \n",
    "        train_loader = DataLoader(train_data_retriever,\n",
    "                                  batch_size=Config.batch_size, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "        valid_loader = DataLoader(valid_data_retriever, \n",
    "                                  batch_size=Config.batch_size * 2, \n",
    "                                  shuffle=False, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        model = Model()\n",
    "        model.to(device,non_blocking=Config.non_blocking)\n",
    "        optimizer = AdamW(model.parameters(), lr=Config.lr,eps=1e-04, weight_decay=Config.weight_decay, amsgrad=False) #eps to avoid NaN/Inf in training loss\n",
    "        scheduler = get_scheduler(optimizer, len(train_X))\n",
    "        best_valid_score = -np.inf\n",
    "        if use_checkpoint:\n",
    "            print(\"Load Checkpoint, epo\")\n",
    "            checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_valid_score = float(checkpoint['best_valid_score'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        \n",
    "        \n",
    "        criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "        \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model, \n",
    "            device, \n",
    "            optimizer, \n",
    "            criterion,\n",
    "            scheduler,\n",
    "            valid_labels,\n",
    "            best_valid_score,\n",
    "            fold\n",
    "        )\n",
    "\n",
    "        history = trainer.fit(\n",
    "            epochs=Config.epochs, \n",
    "            train_loader=train_loader, \n",
    "            valid_loader=valid_loader,\n",
    "            save_path=f'{Config.model_output_folder}/Fold_{fold}_',\n",
    "        )\n",
    "        folds_val_score.append(trainer.best_valid_score)\n",
    "        del train_data_retriever\n",
    "    wandb.finish()\n",
    "    print('folds score:', folds_val_score)\n",
    "    print(\"Avg: {:.5f}\".format(np.mean(folds_val_score)))\n",
    "    print(\"Std: {:.5f}\".format(np.std(folds_val_score)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36681e6-4f7d-4d6c-b54f-57d800d63016",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d285326b-438d-40af-a1db-eb618cf145c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaggle_go\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"1b0833b15e81d54fad9cfbbe3d923f57562a6f89\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d12bd58-7094-43db-85b9-9071b0fecfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">120th_V2_PL_6ep_1em3lr_32ch_vf_s01</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kaggle_go/G2Net\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/ygtl8cym\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/ygtl8cym</a><br/>\n",
       "                Run data is saved locally in <code>/home/wandb/run-20210926_004017-ygtl8cym</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_type= \"debug\" if Config.debug else \"train\"\n",
    "# run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=job_type)\n",
    "run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=Config.model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f8adc-4368-48be-8015-cf9e1e9719ab",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3560abc6-3e87-4021-84e8-abddfd72130a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "skip\n",
      "Fold:  1\n",
      "skip\n",
      "Fold:  2\n",
      "skip\n",
      "Fold:  3\n",
      "skip\n",
      "Fold:  4\n",
      "training data samples, val data samples:  674000 112000\n",
      "566.8512108325958\n",
      "ModelIafossV2\n",
      "Epoch:  0\n",
      "[350/10532]  avg loss:  0.6231238228934152 inst loss:  0.5021153688430786\n",
      "[700/10532]  avg loss:  0.5602088274274554 inst loss:  0.5149758458137512\n",
      "[1050/10532]  avg loss:  0.5261187453497024 inst loss:  0.45295965671539307\n",
      "[1400/10532]  avg loss:  0.5066041346958705 inst loss:  0.47173744440078735\n",
      "[1750/10532]  avg loss:  0.4940908900669643 inst loss:  0.39386022090911865\n",
      "[2100/10532]  avg loss:  0.4840337844122024 inst loss:  0.34334924817085266\n",
      "[2450/10532]  avg loss:  0.47721156529017855 inst loss:  0.3970646858215332\n",
      "[2800/10532]  avg loss:  0.4720044381277902 inst loss:  0.49965038895606995\n",
      "[3150/10532]  avg loss:  0.4670198180183532 inst loss:  0.47665244340896606\n",
      "[3500/10532]  avg loss:  0.4626605398995536 inst loss:  0.414180189371109\n",
      "[3850/10532]  avg loss:  0.4586131227171266 inst loss:  0.3042701184749603\n",
      "[4200/10532]  avg loss:  0.4554226829892113 inst loss:  0.34726962447166443\n",
      "[4550/10532]  avg loss:  0.4523643543956044 inst loss:  0.4092003107070923\n",
      "[4900/10532]  avg loss:  0.4501300422512755 inst loss:  0.40799975395202637\n",
      "[5250/10532]  avg loss:  0.44835965401785716 inst loss:  0.40422990918159485\n",
      "[5600/10532]  avg loss:  0.4465219116210937 inst loss:  0.39280965924263\n",
      "[5950/10532]  avg loss:  0.4447083442752101 inst loss:  0.422091007232666\n",
      "[6300/10532]  avg loss:  0.4430940367683532 inst loss:  0.4541751444339752\n",
      "[6650/10532]  avg loss:  0.4415396278782895 inst loss:  0.41316574811935425\n",
      "[7000/10532]  avg loss:  0.4397132045200893 inst loss:  0.40238404273986816\n",
      "[7350/10532]  avg loss:  0.4383670147746599 inst loss:  0.35219722986221313\n",
      "[7700/10532]  avg loss:  0.43676840249594157 inst loss:  0.47426217794418335\n",
      "[8050/10532]  avg loss:  0.4354746033093944 inst loss:  0.40716487169265747\n",
      "[8400/10532]  avg loss:  0.4345704578218006 inst loss:  0.41911208629608154\n",
      "[8750/10532]  avg loss:  0.4333251116071429 inst loss:  0.5017800331115723\n",
      "[9100/10532]  avg loss:  0.4325187800480769 inst loss:  0.3614419102668762\n",
      "[9450/10532]  avg loss:  0.43148602843915346 inst loss:  0.40454792976379395\n",
      "[9800/10532]  avg loss:  0.43050636758609695 inst loss:  0.367766797542572\n",
      "[10150/10532]  avg loss:  0.4295320678109606 inst loss:  0.4362078905105591\n",
      "[10500/10532]  avg loss:  0.4285984468005952 inst loss:  0.3587731719017029\n",
      "train_loss:  tensor(0.4285, device='cuda:0')\n",
      "valid_loss:  0.4201029091903142\n",
      "valid_score:  0.8727100468913167\n",
      "best_valid_score:  0.8727100468913167\n",
      "time used:  742.1271216869354\n",
      "Epoch:  1\n",
      "[350/10532]  avg loss:  0.40093300955636163 inst loss:  0.46834659576416016\n",
      "[700/10532]  avg loss:  0.4000276402064732 inst loss:  0.38897138833999634\n",
      "[1050/10532]  avg loss:  0.39754438127790176 inst loss:  0.5493202209472656\n",
      "[1400/10532]  avg loss:  0.3980499703543527 inst loss:  0.36992111802101135\n",
      "[1750/10532]  avg loss:  0.39764711216517856 inst loss:  0.45393678545951843\n",
      "[2100/10532]  avg loss:  0.3984740048363095 inst loss:  0.44139939546585083\n",
      "[2450/10532]  avg loss:  0.3985331881776148 inst loss:  0.40160635113716125\n",
      "[2800/10532]  avg loss:  0.39799996512276786 inst loss:  0.3940052390098572\n",
      "[3150/10532]  avg loss:  0.3977783203125 inst loss:  0.38866737484931946\n",
      "[3500/10532]  avg loss:  0.39757924107142856 inst loss:  0.4304622411727905\n",
      "[3850/10532]  avg loss:  0.396839742288961 inst loss:  0.30150166153907776\n",
      "[4200/10532]  avg loss:  0.3964650181361607 inst loss:  0.3880132734775543\n",
      "[4550/10532]  avg loss:  0.3960459681919643 inst loss:  0.439145565032959\n",
      "[4900/10532]  avg loss:  0.3958417619977679 inst loss:  0.4064474105834961\n",
      "[5250/10532]  avg loss:  0.3956553199404762 inst loss:  0.3749018907546997\n",
      "[5600/10532]  avg loss:  0.3954120744977679 inst loss:  0.33942094445228577\n",
      "[5950/10532]  avg loss:  0.3952270712972689 inst loss:  0.37830686569213867\n",
      "[6300/10532]  avg loss:  0.39509017702132937 inst loss:  0.4745294153690338\n",
      "[6650/10532]  avg loss:  0.3951715592692669 inst loss:  0.4307554066181183\n",
      "[7000/10532]  avg loss:  0.3949546595982143 inst loss:  0.3597450852394104\n",
      "[7350/10532]  avg loss:  0.3947369924532313 inst loss:  0.35773056745529175\n",
      "[7700/10532]  avg loss:  0.3946205991274351 inst loss:  0.3586564064025879\n",
      "[8050/10532]  avg loss:  0.3945974257569876 inst loss:  0.3471696376800537\n",
      "[8400/10532]  avg loss:  0.3945136951264881 inst loss:  0.4186435639858246\n",
      "[8750/10532]  avg loss:  0.39441082589285714 inst loss:  0.3736012876033783\n",
      "[9100/10532]  avg loss:  0.39421147943853024 inst loss:  0.5110524892807007\n",
      "[9450/10532]  avg loss:  0.39410481770833333 inst loss:  0.3784332871437073\n",
      "[9800/10532]  avg loss:  0.39387558294802294 inst loss:  0.3838914930820465\n",
      "[10150/10532]  avg loss:  0.3939405259005542 inst loss:  0.41318243741989136\n",
      "[10500/10532]  avg loss:  0.39370335751488095 inst loss:  0.4154532551765442\n",
      "train_loss:  tensor(0.3937, device='cuda:0')\n",
      "valid_loss:  0.42586763262748717\n",
      "valid_score:  0.877393268803916\n",
      "best_valid_score:  0.877393268803916\n",
      "time used:  753.5779640674591\n",
      "Epoch:  2\n",
      "[350/10532]  avg loss:  0.38363045828683034 inst loss:  0.33028846979141235\n",
      "[700/10532]  avg loss:  0.3829733712332589 inst loss:  0.39924708008766174\n",
      "[1050/10532]  avg loss:  0.38528904506138395 inst loss:  0.39098718762397766\n",
      "[1400/10532]  avg loss:  0.38554155622209824 inst loss:  0.42617425322532654\n",
      "[1750/10532]  avg loss:  0.38539205496651785 inst loss:  0.42693379521369934\n",
      "[2100/10532]  avg loss:  0.3857970900762649 inst loss:  0.3144870698451996\n",
      "[2450/10532]  avg loss:  0.38590994698660713 inst loss:  0.47367119789123535\n",
      "[2800/10532]  avg loss:  0.38575099400111607 inst loss:  0.35971543192863464\n",
      "[3150/10532]  avg loss:  0.3856255425347222 inst loss:  0.30891427397727966\n",
      "[3500/10532]  avg loss:  0.38611080496651784 inst loss:  0.4572405517101288\n",
      "[3850/10532]  avg loss:  0.3862319906655844 inst loss:  0.42280977964401245\n",
      "[4200/10532]  avg loss:  0.3861375499906994 inst loss:  0.3836364150047302\n",
      "[4550/10532]  avg loss:  0.3859108591603709 inst loss:  0.3178991377353668\n",
      "[4900/10532]  avg loss:  0.3861408093510842 inst loss:  0.4072933495044708\n",
      "[5250/10532]  avg loss:  0.3862149135044643 inst loss:  0.3981548845767975\n",
      "[5600/10532]  avg loss:  0.3860671561104911 inst loss:  0.40051546692848206\n",
      "[5950/10532]  avg loss:  0.38626715139180673 inst loss:  0.3921658992767334\n",
      "[6300/10532]  avg loss:  0.38635157025049605 inst loss:  0.38933423161506653\n",
      "[6650/10532]  avg loss:  0.38634152519971804 inst loss:  0.39911961555480957\n",
      "[7000/10532]  avg loss:  0.38640673828125 inst loss:  0.30190619826316833\n",
      "[7350/10532]  avg loss:  0.38625617825255104 inst loss:  0.43739381432533264\n",
      "[7700/10532]  avg loss:  0.3860329684963474 inst loss:  0.3431667685508728\n",
      "[8050/10532]  avg loss:  0.38589531371312114 inst loss:  0.44483911991119385\n",
      "[8400/10532]  avg loss:  0.38588158017113094 inst loss:  0.38339418172836304\n",
      "[8750/10532]  avg loss:  0.3857652064732143 inst loss:  0.4008525609970093\n",
      "[9100/10532]  avg loss:  0.38543068015968407 inst loss:  0.4312715232372284\n",
      "[9450/10532]  avg loss:  0.3853183128720238 inst loss:  0.4273003339767456\n",
      "[9800/10532]  avg loss:  0.38529740314094385 inst loss:  0.4232823848724365\n",
      "[10150/10532]  avg loss:  0.38535096116841133 inst loss:  0.3607211112976074\n",
      "[10500/10532]  avg loss:  0.38542996651785716 inst loss:  0.35832858085632324\n",
      "train_loss:  tensor(0.3854, device='cuda:0')\n",
      "valid_loss:  0.4030374813420432\n",
      "valid_score:  0.878996724101836\n",
      "best_valid_score:  0.878996724101836\n",
      "time used:  721.7744677066803\n",
      "Epoch:  3\n",
      "[350/10532]  avg loss:  0.38184042794363837 inst loss:  0.3278422951698303\n",
      "[700/10532]  avg loss:  0.38312329973493303 inst loss:  0.28683289885520935\n",
      "[1050/10532]  avg loss:  0.38238542829241073 inst loss:  0.3688913881778717\n",
      "[1400/10532]  avg loss:  0.3819736589704241 inst loss:  0.38538920879364014\n",
      "[1750/10532]  avg loss:  0.38247303989955356 inst loss:  0.5697652101516724\n",
      "[2100/10532]  avg loss:  0.38219976515997023 inst loss:  0.32663819193840027\n",
      "[2450/10532]  avg loss:  0.3820074836575255 inst loss:  0.31780022382736206\n",
      "[2800/10532]  avg loss:  0.3816323416573661 inst loss:  0.4150773286819458\n",
      "[3150/10532]  avg loss:  0.3817068917410714 inst loss:  0.2882647216320038\n",
      "[3500/10532]  avg loss:  0.3819619489397321 inst loss:  0.3463888168334961\n",
      "[3850/10532]  avg loss:  0.381623440036526 inst loss:  0.2723398804664612\n",
      "[4200/10532]  avg loss:  0.3811912318638393 inst loss:  0.35335779190063477\n",
      "[4550/10532]  avg loss:  0.381299311040522 inst loss:  0.4465770721435547\n",
      "[4900/10532]  avg loss:  0.38124618841677294 inst loss:  0.31313571333885193\n",
      "[5250/10532]  avg loss:  0.381439453125 inst loss:  0.3971254825592041\n",
      "[5600/10532]  avg loss:  0.38124904087611605 inst loss:  0.5041248798370361\n",
      "[5950/10532]  avg loss:  0.38110372078518906 inst loss:  0.3367210328578949\n",
      "[6300/10532]  avg loss:  0.38088731553819444 inst loss:  0.37236571311950684\n",
      "[6650/10532]  avg loss:  0.3807461524906015 inst loss:  0.28950953483581543\n",
      "[7000/10532]  avg loss:  0.3808353445870536 inst loss:  0.48156607151031494\n",
      "[7350/10532]  avg loss:  0.3805208665497449 inst loss:  0.380191832780838\n",
      "[7700/10532]  avg loss:  0.38043101917613636 inst loss:  0.2727433145046234\n",
      "[8050/10532]  avg loss:  0.38036141910908383 inst loss:  0.4575662612915039\n",
      "[8400/10532]  avg loss:  0.3804012625558036 inst loss:  0.3594515323638916\n",
      "[8750/10532]  avg loss:  0.3805288783482143 inst loss:  0.36815664172172546\n",
      "[9100/10532]  avg loss:  0.3805755279876374 inst loss:  0.33023104071617126\n",
      "[9450/10532]  avg loss:  0.3804664248511905 inst loss:  0.3825884461402893\n",
      "[9800/10532]  avg loss:  0.38031117964764033 inst loss:  0.33931803703308105\n",
      "[10150/10532]  avg loss:  0.38026703933189654 inst loss:  0.45046111941337585\n",
      "[10500/10532]  avg loss:  0.3800956333705357 inst loss:  0.4253402352333069\n",
      "train_loss:  tensor(0.3801, device='cuda:0')\n",
      "valid_loss:  0.4078975996630532\n",
      "valid_score:  0.8801753225492229\n",
      "best_valid_score:  0.8801753225492229\n",
      "time used:  742.8895111083984\n",
      "Epoch:  4\n",
      "[350/10532]  avg loss:  0.37864702497209823 inst loss:  0.32332298159599304\n",
      "[700/10532]  avg loss:  0.37722098214285715 inst loss:  0.3689175844192505\n",
      "[1050/10532]  avg loss:  0.3782153610956101 inst loss:  0.31288623809814453\n",
      "[1400/10532]  avg loss:  0.3765993826729911 inst loss:  0.36543405055999756\n",
      "[1750/10532]  avg loss:  0.3761534946986607 inst loss:  0.3747459053993225\n",
      "[2100/10532]  avg loss:  0.37635108584449406 inst loss:  0.38835370540618896\n",
      "[2450/10532]  avg loss:  0.37631631656568876 inst loss:  0.3002760410308838\n",
      "[2800/10532]  avg loss:  0.37688153948102676 inst loss:  0.39060720801353455\n",
      "[3150/10532]  avg loss:  0.3761846633184524 inst loss:  0.3461533784866333\n",
      "[3500/10532]  avg loss:  0.37647659737723216 inst loss:  0.4653922915458679\n",
      "[3850/10532]  avg loss:  0.3766168450689935 inst loss:  0.4264451861381531\n",
      "[4200/10532]  avg loss:  0.37679242815290176 inst loss:  0.3629304766654968\n",
      "[4550/10532]  avg loss:  0.37681664770776097 inst loss:  0.3916316628456116\n",
      "[4900/10532]  avg loss:  0.37660029197225764 inst loss:  0.3453676104545593\n",
      "[5250/10532]  avg loss:  0.3766982421875 inst loss:  0.477742075920105\n",
      "[5600/10532]  avg loss:  0.3767885480608259 inst loss:  0.40085434913635254\n",
      "[5950/10532]  avg loss:  0.37690064502363446 inst loss:  0.3570184111595154\n",
      "[6300/10532]  avg loss:  0.37689429873511904 inst loss:  0.5425885915756226\n",
      "[6650/10532]  avg loss:  0.37695338199013156 inst loss:  0.3502101004123688\n",
      "[7000/10532]  avg loss:  0.3768193359375 inst loss:  0.5066550970077515\n",
      "[7350/10532]  avg loss:  0.37669230973639456 inst loss:  0.5116606950759888\n",
      "[7700/10532]  avg loss:  0.3765818410105519 inst loss:  0.4398328363895416\n",
      "[8050/10532]  avg loss:  0.3765843665081522 inst loss:  0.41357195377349854\n",
      "[8400/10532]  avg loss:  0.3764724876767113 inst loss:  0.4084587097167969\n",
      "[8750/10532]  avg loss:  0.3763970145089286 inst loss:  0.36699384450912476\n",
      "[9100/10532]  avg loss:  0.3764534684065934 inst loss:  0.3912639021873474\n",
      "[9450/10532]  avg loss:  0.3762822162285053 inst loss:  0.3799086809158325\n",
      "[9800/10532]  avg loss:  0.3762757344148597 inst loss:  0.36107462644577026\n",
      "[10150/10532]  avg loss:  0.3761448872383005 inst loss:  0.36950668692588806\n",
      "[10500/10532]  avg loss:  0.3762047526041667 inst loss:  0.3327091932296753\n",
      "train_loss:  tensor(0.3762, device='cuda:0')\n",
      "valid_loss:  0.409589180128915\n",
      "valid_score:  0.880447098076413\n",
      "best_valid_score:  0.880447098076413\n",
      "time used:  719.2325625419617\n",
      "Epoch:  5\n",
      "[350/10532]  avg loss:  0.3782099260602679 inst loss:  0.2817247807979584\n",
      "[700/10532]  avg loss:  0.3743365042550223 inst loss:  0.3638879358768463\n",
      "[1050/10532]  avg loss:  0.37503362746465774 inst loss:  0.3387097716331482\n",
      "[1400/10532]  avg loss:  0.3746435982840402 inst loss:  0.3353281021118164\n",
      "[1750/10532]  avg loss:  0.37308335658482145 inst loss:  0.3474613428115845\n",
      "[2100/10532]  avg loss:  0.3733894275483631 inst loss:  0.42669248580932617\n",
      "[2450/10532]  avg loss:  0.3730576869419643 inst loss:  0.3843086361885071\n",
      "[2800/10532]  avg loss:  0.37262202671595984 inst loss:  0.28643810749053955\n",
      "[3150/10532]  avg loss:  0.37268965463789683 inst loss:  0.34597593545913696\n",
      "[3500/10532]  avg loss:  0.3726339285714286 inst loss:  0.38526397943496704\n",
      "[3850/10532]  avg loss:  0.3729292119013799 inst loss:  0.4626508057117462\n",
      "[4200/10532]  avg loss:  0.37356026785714286 inst loss:  0.5363806486129761\n",
      "[4550/10532]  avg loss:  0.37386657044127747 inst loss:  0.381350576877594\n",
      "[4900/10532]  avg loss:  0.37398133569834185 inst loss:  0.2928483486175537\n",
      "[5250/10532]  avg loss:  0.3740043480282738 inst loss:  0.3782883882522583\n",
      "[5600/10532]  avg loss:  0.37400499616350447 inst loss:  0.40155738592147827\n",
      "[5950/10532]  avg loss:  0.3738975512079832 inst loss:  0.3523130416870117\n",
      "[6300/10532]  avg loss:  0.37386947389632935 inst loss:  0.34161168336868286\n",
      "[6650/10532]  avg loss:  0.37408860285479323 inst loss:  0.4271930456161499\n",
      "[7000/10532]  avg loss:  0.37402859933035715 inst loss:  0.317025363445282\n",
      "[7350/10532]  avg loss:  0.37381145036139457 inst loss:  0.29872798919677734\n",
      "[7700/10532]  avg loss:  0.37361321783685064 inst loss:  0.31628766655921936\n",
      "[8050/10532]  avg loss:  0.37363451086956523 inst loss:  0.2693902254104614\n",
      "[8400/10532]  avg loss:  0.3735721551804315 inst loss:  0.4481663107872009\n",
      "[8750/10532]  avg loss:  0.3735091517857143 inst loss:  0.4290873408317566\n",
      "[9100/10532]  avg loss:  0.3734783063616071 inst loss:  0.40219271183013916\n",
      "[9450/10532]  avg loss:  0.37343235883763226 inst loss:  0.3630495071411133\n",
      "[9800/10532]  avg loss:  0.373261793486926 inst loss:  0.312921941280365\n",
      "[10150/10532]  avg loss:  0.37319574064809113 inst loss:  0.38905954360961914\n",
      "[10500/10532]  avg loss:  0.3731409272693452 inst loss:  0.38510116934776306\n",
      "train_loss:  tensor(0.3732, device='cuda:0')\n",
      "valid_loss:  0.5212649734701429\n",
      "valid_score:  0.8793222746642747\n",
      "best_valid_score:  0.880447098076413\n",
      "time used:  735.8630003929138\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 401<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/wandb/run-20210926_004017-ygtl8cym/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/wandb/run-20210926_004017-ygtl8cym/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold4] avg_train_loss</td><td>0.37316</td></tr><tr><td>[fold4] avg_val_loss</td><td>0.52126</td></tr><tr><td>[fold4] epoch</td><td>6</td></tr><tr><td>[fold4] loss</td><td>0.35528</td></tr><tr><td>[fold4] lr</td><td>0.0</td></tr><tr><td>[fold4] val_score</td><td>0.87932</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold4] avg_train_loss</td><td>â–ˆâ–„â–ƒâ–‚â–â–</td></tr><tr><td>[fold4] avg_val_loss</td><td>â–‚â–‚â–â–â–â–ˆ</td></tr><tr><td>[fold4] epoch</td><td>â–â–‚â–„â–…â–‡â–ˆ</td></tr><tr><td>[fold4] loss</td><td>â–ˆâ–†â–ƒâ–†â–„â–‡â–…â–†â–…â–ƒâ–„â–…â–„â–ƒâ–‡â–‚â–‚â–ƒâ–‚â–ƒâ–„â–‚â–…â–„â–†â–â–†â–â–‚â–‡â–…â–…â–ƒâ–„â–„â–…â–„â–ƒâ–„â–…</td></tr><tr><td>[fold4] lr</td><td>â–‚â–ƒâ–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–</td></tr><tr><td>[fold4] val_score</td><td>â–â–…â–‡â–ˆâ–ˆâ–‡</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">120th_V2_PL_6ep_1em3lr_32ch_vf_s01</strong>: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/ygtl8cym\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/ygtl8cym</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folds score: [0.880447098076413]\n",
      "Avg: 0.88045\n",
      "Std: 0.00000\n",
      "CPU times: user 2h 33min 43s, sys: 5h 46min 12s, total: 8h 19min 55s\n",
      "Wall time: 1h 25min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "#     %lprun -f DataRetriever.__getitem__ -f Trainer.train_epoch -f Trainer.fit -f Trainer.valid_epoch training_loop() \n",
    "    training_loop(train_df,Config.use_checkpoint)\n",
    "except RuntimeError as e:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()   \n",
    "    print(e)# saving oof predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a03bfac-e977-4cc1-908f-fd91ad857903",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eec60e-00c2-43aa-9768-8fb74482de2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d39375ae-356c-4742-8f68-f020b8d71fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "print(Config.train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce12376c-30df-443b-b057-3a54f4c75230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# import Ipython\n",
    "# IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc76b4-47ca-4622-a4c1-0763f96aaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80731b-2b25-48c4-83bc-491572148207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarviscloud import jarviscloud\n",
    "jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b00fb94-6a9f-43f0-b73d-5738a7e48439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "successfully saved oof predictions for Fold:  0\n",
      "1\n",
      "successfully saved oof predictions for Fold:  1\n",
      "2\n",
      "successfully saved oof predictions for Fold:  2\n",
      "3\n",
      "successfully saved oof predictions for Fold:  3\n",
      "4\n",
      "successfully saved oof predictions for Fold:  4\n"
     ]
    }
   ],
   "source": [
    "for fold in Config.train_folds:\n",
    "    print(fold)\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    # print(checkpoint['valid_preds'])\n",
    "    try:\n",
    "        # oof = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv') also works, used in replacement of next statement for previously not generated Fold_{fold}_oof_pred.csv\n",
    "        oof = pd.read_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        oof['pred'] = checkpoint['valid_preds']\n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv') \n",
    "        print('successfully saved oof predictions for Fold: ', fold)   \n",
    "    except:\n",
    "        raise RuntimeError('failure in saving predictions for Fold: ', fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb59e1-9caf-4636-bb62-18c237d5c716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6afe6c-0be0-485b-a2e2-a899024b8a80",
   "metadata": {},
   "source": [
    "# add TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe5ee8-3a67-43fd-94f8-4f04984c9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e70a9794-653f-4c95-9108-547bf07f2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tbs need pythonic way\n",
    "class TTA(Dataset):\n",
    "    def __init__(self, paths, targets, vflip=False, shuffle_channels=False, time_shift=False, \n",
    "                 add_gaussian_noise = False,  time_stretch=False,shuffle01=False,timemask=False,\n",
    "                 shift_channel=False,reduce_SNR=False, ):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.vflip = vflip\n",
    "        self.shuffle_channels = shuffle_channels\n",
    "        self.time_shift = time_shift\n",
    "        self.add_gaussian_noise = add_gaussian_noise\n",
    "        self.time_stretch = time_stretch\n",
    "        self.shuffle01 = shuffle01\n",
    "        self.timemask = timemask\n",
    "        self.shift_channel = shift_channel\n",
    "        self.reduce_SNR = reduce_SNR\n",
    "        if time_shift:\n",
    "            self.time_shift = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096, \n",
    "                                      max_fraction=Config.time_shift_right*1.0/4096, p=1,rollover=False)\n",
    "        if add_gaussian_noise:\n",
    "            self.add_gaussian_noise = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude= 0.015*0.015, p=1)\n",
    "        if time_stretch:\n",
    "            self.time_stretch = A.TimeStretch(min_rate=0.9, max_rate=1.111,leave_length_unchanged=True, p=1)\n",
    "        if timemask:\n",
    "            self.timemask = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=1.0)\n",
    "\n",
    "              \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "        if self.vflip:\n",
    "            waves = -waves\n",
    "        if self.shuffle_channels:\n",
    "            np.random.shuffle(waves)\n",
    "        if self.time_shift:\n",
    "            waves = self.time_shift(waves, sample_rate=2048)\n",
    "        if self.add_gaussian_noise:\n",
    "            waves = self.add_gaussian_noise(waves, sample_rate=2048)\n",
    "        if self.time_stretch:\n",
    "            waves = self.time_stretch(waves, sample_rate=2048)\n",
    "        if self.shuffle01:\n",
    "            waves[[0,1]] = waves[[1,0]]\n",
    "        if self.timemask:\n",
    "            waves = self.timemask(waves, sample_rate=2048)\n",
    "        if self.shift_channel:\n",
    "            waves = shift_channel_func(waves, sample_rate=2048)\n",
    "        if self.reduce_SNR:\n",
    "            waves = reduce_SNR_func(waves, sample_rate=2048)\n",
    "        #snr, shift_channel tba\n",
    "        \n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device,             \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30c18af1-daaf-409d-bbe8-180ca6db1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e95c39f-c917-4d9b-afe8-dc89d5d8f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(loader,model):\n",
    "    preds = []\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        if step % Config.print_num_steps == 0:\n",
    "            print(\"step {}/{}\".format(step, len(loader)))\n",
    "        with torch.no_grad():\n",
    "            X = batch[0].to(device,non_blocking=Config.non_blocking)\n",
    "            outputs = model(X).squeeze()\n",
    "            preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def get_tta_pred(df,model,**transforms):\n",
    "    data_retriever = TTA(df['file_path'].values, df['target'].values, **transforms)\n",
    "    loader = DataLoader(data_retriever, \n",
    "                            batch_size=Config.batch_size * 2, \n",
    "                            shuffle=False, \n",
    "                            num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "    return get_pred(loader,model)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f838287-0fa6-442d-b39d-f700b6ea483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['vflip', 'shuffle01']\n"
     ]
    }
   ],
   "source": [
    "##TTA for oof\n",
    "print(conserv_transform_list_strings)\n",
    "print(aggressive_transform_list_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "709164c2-28c6-4eed-954e-3fc524bc9fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[()]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "conserv_transform_powerset = list(powerset(conserv_transform_list_strings))\n",
    "conserv_transform_powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7343696-bcac-4160-8a76-0d79c0dee4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "for transformations in conserv_transform_powerset:\n",
    "    print({transformation:True for transformation in transformations})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5e8f7-1349-45fe-b646-42e0febed1c3",
   "metadata": {},
   "source": [
    "## generate oof tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c09fa2a-7afc-4c7d-9fbe-fa02890d73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n",
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n",
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/875\n",
      "step 700/875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Model()\n",
    "\n",
    "for fold in Config.train_folds:\n",
    "    print('Fold ',fold)\n",
    "    oof = train_df.query(f\"fold=={fold}\").copy()\n",
    "    oof['preds'] = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')['valid_preds']\n",
    "    oof['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "    # display(oof)    \n",
    "\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "    \n",
    "    for transformations in conserv_transform_powerset:\n",
    "#         print(transformations)\n",
    "        if transformations:#to avoid double count original\n",
    "            print(\"tta_\"+('_').join(transformations))\n",
    "            oof[\"tta_\"+('_').join(transformations)] = get_tta_pred(oof,model,**{transformation:True for transformation in transformations})\n",
    "        for aggr_transformation in aggressive_transform_list_strings:#tbs combination of conservative and aggressive\n",
    "            print(\"tta_\"+('_').join(transformations)+'_'+aggr_transformation)\n",
    "            oof[\"tta_\"+('_').join(transformations)+'_'+aggr_transformation] = get_tta_pred(oof,model,**{transformation:True for transformation in transformations}, **{aggr_transformation:True})\n",
    "               \n",
    "\n",
    "    oof.to_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd3fa0d4-3e28-48ea-bf7a-fa6c2df077f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_all = pd.DataFrame()\n",
    "for fold in Config.train_folds:\n",
    "    oof = pd.read_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\")\n",
    "    oof_all = pd.concat([oof_all,oof])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ca818-92aa-4734-8a20-874db80b6c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed70f5fd-00b8-4eb9-a003-6fec6de253d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('_').join(transformations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da1a3065-5238-4efa-8851-cb60c17bc878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.880868075577214\n",
      "tta__vflip 0.880803035585904\n",
      "tta__shuffle01 0.8806446858885072\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",roc_auc_score(oof_all['target'], oof_all['preds']))\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    if \"tta\" in col:\n",
    "        print(col,roc_auc_score(oof_all['target'], oof_all[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2757f84-5698-4982-8d06-c3058bc0f410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8805179780234451"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_sample = oof_all[oof_all['fold']==2]\n",
    "roc_auc_score(oof_sample['target'], oof_sample['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b324aba-a293-43a2-86e5-9fae454daa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f88035d2ee0>, {'preds': 0.25, 'tta__vflip': 0.375, 'tta__shuffle01': 0.375})\n",
      "preds 0.25\n",
      "tta__vflip 0.375\n",
      "tta__shuffle01 0.375\n",
      "preds\n",
      "tta__vflip\n",
      "tta__shuffle01\n",
      "preds_tta_avg: 0.8810127677673796\n"
     ]
    }
   ],
   "source": [
    "oof_all['avg']=0\n",
    "total_weight = 0\n",
    "#weights leaky? not fine tuned\n",
    "\n",
    "oof_weight  = defaultdict(lambda :1)\n",
    "aggr_total_weight = 0\n",
    "for trans in aggressive_transform_list_strings:\n",
    "    aggr_total_weight += getattr(Config(),trans+'_weight')\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    \n",
    "    if 'tta_' in col or 'preds' in col: \n",
    "        for trans in conserv_transform_list_strings:\n",
    "            \n",
    "            if trans in col:\n",
    "                oof_weight[col] *= getattr(Config(),trans+'_proba')\n",
    "            else:\n",
    "                oof_weight[col] *= 1-getattr(Config(),trans+'_proba')\n",
    "            \n",
    "        flag = False\n",
    "        for trans in aggressive_transform_list_strings:\n",
    "            \n",
    "            if trans in col:\n",
    "                oof_weight[col] *= getattr(Config(),trans+'_weight')/aggr_total_weight*Config.aggressive_aug_proba\n",
    "                \n",
    "                flag = True\n",
    "        if not flag:\n",
    "            oof_weight[col] *= (1-Config.aggressive_aug_proba)\n",
    "        \n",
    "print(oof_weight)\n",
    "for key,value in oof_weight.items():\n",
    "    print(key,value)\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    if ('tta_' in col or 'preds' in col): # and 'time_shift' not in col and 'timemask' not in col\n",
    "        print(col)\n",
    "        total_weight+=oof_weight[col]\n",
    "        oof_all['avg'] += oof_all[col]*oof_weight[col]\n",
    "oof_all['avg'] /= total_weight\n",
    "\n",
    "print(\"preds_tta_avg:\",roc_auc_score(oof_all['target'], oof_all['avg']))\n",
    "\n",
    "oof_all.to_csv(Config.model_output_folder + \"/oof_all.csv\", index=False)\n",
    "oof_all[['id','fold','avg']].rename(columns={'id':'id','fold':'fold','avg':'prediction'}).to_csv(Config.model_output_folder + \"/oof_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a7c77-63b3-4efd-82e3-588ff224c583",
   "metadata": {},
   "source": [
    "## generate TTA for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b0a1c70-e352-435b-8767-3c397d4bd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n",
      "tta__vflip_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n",
      "tta__vflip_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n",
      "tta__shuffle01_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n",
      "tta__vflip_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n",
      "tta__vflip_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n",
      "tta__shuffle01_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n",
      "tta__vflip_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n",
      "tta__shuffle01_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/1766\n",
      "step 700/1766\n",
      "step 1050/1766\n",
      "step 1400/1766\n",
      "step 1750/1766\n",
      "CPU times: user 9min 19s, sys: 1min 8s, total: 10min 28s\n",
      "Wall time: 9min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "test_df['target'] = 0  \n",
    "model = Model()\n",
    "\n",
    "for fold in Config.train_folds:\n",
    "    test_df2 = test_df.copy()\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "\n",
    "    test_df2['preds'+f'_Fold_{fold}'] = get_tta_pred(test_df2,model)\n",
    "\n",
    "    for transformations in conserv_transform_powerset:\n",
    "#         print(transformations)\n",
    "        if transformations:#to avoid double count original\n",
    "            print(\"tta_\"+('_').join(transformations)+f'_Fold_{fold}')\n",
    "            test_df2[\"tta_\"+('_').join(transformations)+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,**{transformation:True for transformation in transformations})\n",
    "        for transformation in aggressive_transform_list_strings:#tbs combination of conservative and aggressive\n",
    "            print(\"tta_\"+('_').join(transformations)+'_'+transformation+f'_Fold_{fold}')\n",
    "            test_df2[\"tta_\"+('_').join(transformations)+'_'+transformation+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,**{transformation:True for transformation in transformations}, **{transformation:True})\n",
    "               \n",
    "    test_df2.to_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1eab6a61-d4eb-447e-ae1f-33b1af2958ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              target\n",
      "count  226000.000000\n",
      "mean        0.428614\n",
      "std         0.380893\n",
      "min         0.014408\n",
      "25%         0.099668\n",
      "50%         0.236500\n",
      "75%         0.941777\n",
      "max         1.000000\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n",
      "                id    target\n",
      "0       00005bced6  0.999879\n",
      "1       0000806717  0.930308\n",
      "2       0000ef4fe1  0.217689\n",
      "3       00020de251  0.949399\n",
      "4       00024887b5  0.054091\n",
      "...            ...       ...\n",
      "225995  ffff4125f1  0.152698\n",
      "225996  ffff9d32a6  0.161651\n",
      "225997  ffff9f4c1f  0.127274\n",
      "225998  ffffa19693  0.999968\n",
      "225999  ffffebbfe2  0.047218\n",
      "\n",
      "[226000 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD5CAYAAADFqlkBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAASnElEQVR4nO3cfYxc1XnH8e8TOyTUgWDiZIVsN6aK09YBJYEVOErVbkMLi1NhpBIEaoKN3FgKUKWt1dZpK9FCkKBVQ4NEXtxg2URNgKZNWQVT1wKPUKsaDKEhGErZOCS2S+IGG5MNCunSp3/MsTvd7nqu2Z2Z3bnfjzTac8899855/LK/PffenchMJEn19rpeT0CS1HuGgSTJMJAkGQaSJAwDSRKGgSQJmF9lUEQ8B/wQeBUYz8zBiDgduBtYBjwHXJ6ZhyMigE8Dq4CXgbWZ+fVynjXAH5fTfjIzt5b+c4EtwMnANuDj2eaZ10WLFuWyZcvazv1HP/oRCxYsqFJmX7Hueqlj3XWsGaZX92OPPfaDzHzrpDszs+2L5jf7RRP6/gzYWNobgVtKexVwPxDASuDh0n86sLd8XVjaC8u+R8rYKMde3G5O5557blaxc+fOSuP6jXXXSx3rrmPNmdOrG3g0p/ieOp3LRKuBraW9Fbi0pf/O8t67gNMi4gzgImBHZh7KzMPADmC47Ds1M3eVyd7Zci5JUhdUukwEJPCPEZHA5zNzEzCQmc+X/d8DBkp7MbCv5dj9pe94/fsn6f9/ImI9sB5gYGCARqPRduJjY2OVxvUb666XOtZdx5qhc3VXDYNfyMwDEfE2YEdE/FvrzszMEhQdVUJoE8Dg4GAODQ21PabRaFBlXL+x7nqpY911rBk6V3ely0SZeaB8PQh8FTgP+H65xEP5erAMPwAsbTl8Sek7Xv+SSfolSV3SNgwiYkFEnHK0DVwIPAmMAGvKsDXAvaU9AlwVTSuBI+Vy0nbgwohYGBELy3m2l30vRcTK8iTSVS3nkiR1QZXLRAPAV5vfp5kPfCkz/yEidgP3RMQ64DvA5WX8NppPFI3SfLT0aoDMPBQRNwK7y7gbMvNQaV/D/z5aen95SZK6pG0YZOZe4N2T9L8AXDBJfwLXTnGuzcDmSfofBc6qMF9JUgf4G8iSJMNAklT90VJJUo8s23jfsfaW4c58BIcrA0mSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAksQJhEFEzIuIxyPia2X7zIh4OCJGI+LuiDip9L+hbI+W/ctazvGJ0v9MRFzU0j9c+kYjYuMM1idJquBEVgYfB55u2b4FuDUz3wEcBtaV/nXA4dJ/axlHRKwArgDeBQwDnykBMw+4HbgYWAFcWcZKkrqkUhhExBLgg8AXynYAHwC+UoZsBS4t7dVlm7L/gjJ+NXBXZr6Smd8GRoHzyms0M/dm5k+Au8pYSVKXzK847i+B3wdOKdtvAV7MzPGyvR9YXNqLgX0AmTkeEUfK+MXArpZzth6zb0L/+ZNNIiLWA+sBBgYGaDQabSc+NjZWaVy/se56qWPddap5w9njx9qdqrttGETErwEHM/OxiBia8RmcgMzcBGwCGBwczKGh9tNpNBpUGddvrLte6lh3nWpeu/G+Y+0twws6UneVlcH7gUsiYhXwRuBU4NPAaRExv6wOlgAHyvgDwFJgf0TMB94MvNDSf1TrMVP1S5K6oO09g8z8RGYuycxlNG8AP5iZvwHsBC4rw9YA95b2SNmm7H8wM7P0X1GeNjoTWA48AuwGlpenk04q7zEyI9VJkiqpes9gMn8A3BURnwQeB+4o/XcAX4yIUeAQzW/uZOaeiLgHeAoYB67NzFcBIuI6YDswD9icmXumMS9J0gk6oTDIzAbQKO29NJ8Emjjmx8CHpjj+JuCmSfq3AdtOZC6SpJnjbyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJJEhTCIiDdGxCMR8Y2I2BMRf1r6z4yIhyNiNCLujoiTSv8byvZo2b+s5VyfKP3PRMRFLf3DpW80IjZ2oE5J0nFUWRm8AnwgM98NvAcYjoiVwC3ArZn5DuAwsK6MXwccLv23lnFExArgCuBdwDDwmYiYFxHzgNuBi4EVwJVlrCSpS9qGQTaNlc3Xl1cCHwC+Uvq3ApeW9uqyTdl/QURE6b8rM1/JzG8Do8B55TWamXsz8yfAXWWsJKlL5lcZVH56fwx4B82f4r8FvJiZ42XIfmBxaS8G9gFk5nhEHAHeUvp3tZy29Zh9E/rPn2Ie64H1AAMDAzQajbZzHxsbqzSu31h3vdSx7jrVvOHs8WPtTtVdKQwy81XgPRFxGvBV4OdmfCbV5rEJ2AQwODiYQ0NDbY9pNBpUGddvrLte6lh3nWpeu/G+Y+0twws6UvcJPU2UmS8CO4H3AadFxNEwWQIcKO0DwFKAsv/NwAut/ROOmapfktQlVZ4memtZERARJwO/CjxNMxQuK8PWAPeW9kjZpux/MDOz9F9RnjY6E1gOPALsBpaXp5NOonmTeWQGapMkVVTlMtEZwNZy3+B1wD2Z+bWIeAq4KyI+CTwO3FHG3wF8MSJGgUM0v7mTmXsi4h7gKWAcuLZcfiIirgO2A/OAzZm5Z8YqlCS11TYMMvMJ4L2T9O+l+STQxP4fAx+a4lw3ATdN0r8N2FZhvpKkDvA3kCVJhoEkyTCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSaJCGETE0ojYGRFPRcSeiPh46T89InZExLPl68LSHxFxW0SMRsQTEXFOy7nWlPHPRsSalv5zI+Kb5ZjbIiI6UawkaXJVVgbjwIbMXAGsBK6NiBXARuCBzFwOPFC2AS4GlpfXeuCz0AwP4HrgfOA84PqjAVLGfLTluOHplyZJqqptGGTm85n59dL+IfA0sBhYDWwtw7YCl5b2auDObNoFnBYRZwAXATsy81BmHgZ2AMNl36mZuSszE7iz5VySpC6YfyKDI2IZ8F7gYWAgM58vu74HDJT2YmBfy2H7S9/x+vdP0j/Z+6+nudpgYGCARqPRds5jY2OVxvUb666XOtZdp5o3nD1+rN2puiuHQUS8Cfhb4Lcz86XWy/qZmRGRMz67CTJzE7AJYHBwMIeGhtoe02g0qDKu31h3vdSx7jrVvHbjfcfaW4YXdKTuSk8TRcTraQbBX2fm35Xu75dLPJSvB0v/AWBpy+FLSt/x+pdM0i9J6pIqTxMFcAfwdGZ+qmXXCHD0iaA1wL0t/VeVp4pWAkfK5aTtwIURsbDcOL4Q2F72vRQRK8t7XdVyLklSF1S5TPR+4CPANyPiX0vfHwI3A/dExDrgO8DlZd82YBUwCrwMXA2QmYci4kZgdxl3Q2YeKu1rgC3AycD95SVJ6pK2YZCZ/wRM9dz/BZOMT+DaKc61Gdg8Sf+jwFnt5iJJ6gx/A1mSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQqhEFEbI6IgxHxZEvf6RGxIyKeLV8Xlv6IiNsiYjQinoiIc1qOWVPGPxsRa1r6z42Ib5ZjbouImOkiJUnHV2VlsAUYntC3EXggM5cDD5RtgIuB5eW1HvgsNMMDuB44HzgPuP5ogJQxH205buJ7SZI6rG0YZOZDwKEJ3auBraW9Fbi0pf/ObNoFnBYRZwAXATsy81BmHgZ2AMNl36mZuSszE7iz5VySpC6Z/xqPG8jM50v7e8BAaS8G9rWM21/6jte/f5L+SUXEeporDgYGBmg0Gm0nOjY2Vmlcv7Hueqlj3XWqecPZ48fanar7tYbBMZmZEZEzMZkK77UJ2AQwODiYQ0NDbY9pNBpUGddvrLte6lh3nWpeu/G+Y+0twws6UvdrfZro++USD+XrwdJ/AFjaMm5J6Tte/5JJ+iVJXfRaw2AEOPpE0Brg3pb+q8pTRSuBI+Vy0nbgwohYWG4cXwhsL/teioiV5Smiq1rOJUnqkraXiSLiy8AQsCgi9tN8Kuhm4J6IWAd8B7i8DN8GrAJGgZeBqwEy81BE3AjsLuNuyMyjN6WvofnE0snA/eU1ay1rWa49d/MHezgTSZo5bcMgM6+cYtcFk4xN4NopzrMZ2DxJ/6PAWe3m0UutASBJ/cjfQJYkTf9pojrzkpGkfmEYTOFELw0ZDJLmMi8TSZIMA0mSYSBJwnsGHeH9A0lzjSsDSZIrg05zlSBpLnBlIEkyDCRJXib6Pzr9GUReMpI0W7kykCQZBpIkLxP1jJeMJM0mrgwkSa4MZgNXCZJ6zTCYZQwGSb3gZSJJkiuD2cxVgqRucWUgSXJlMFe4SpDUSYbBHDTxYzMMB0nTZRj0AVcNUv/p9GelTVT7MOj2H3inHa1nw9njDPV2KpLmkNqHQT+bKuhcPUiayKeJJEmuDOrIFYOkiQwDHWNISPVlGKitE73JbnhIr00vH2gxDDTjXGFIc49hoK6p8lOPgaG6mS2PtxsGmlWm8x/DINFcMVsCoJVhoL6xbON9bDh7nLUz/B/NkNFrNRu/6U9l1oRBRAwDnwbmAV/IzJt7PCUJ6N1/6KlCyI8f6by59E18psyKMIiIecDtwK8C+4HdETGSmU914v3q+BetuafKv9PWjx+Z6RXRbFfHmjtptvwG8nnAaGbuzcyfAHcBq3s8J0mqjcjMXs+BiLgMGM7M3yzbHwHOz8zrJoxbD6wvmz8LPFPh9IuAH8zgdOcK666XOtZdx5phenW/PTPfOtmOWXGZqKrM3ARsOpFjIuLRzBzs0JRmLeuulzrWXceaoXN1z5bLRAeApS3bS0qfJKkLZksY7AaWR8SZEXEScAUw0uM5SVJtzIrLRJk5HhHXAdtpPlq6OTP3zNDpT+iyUh+x7nqpY911rBk6VPesuIEsSeqt2XKZSJLUQ4aBJKl/wiAihiPimYgYjYiNk+x/Q0TcXfY/HBHLejDNGVeh7t+NiKci4omIeCAi3t6Lec60dnW3jPv1iMiImPOPIFapOSIuL3/feyLiS92eYydU+Df+0xGxMyIeL//OV/VinjMpIjZHxMGIeHKK/RERt5U/kyci4pxpv2lmzvkXzZvO3wJ+BjgJ+AawYsKYa4DPlfYVwN29nneX6v5l4KdK+2N1qbuMOwV4CNgFDPZ63l34u14OPA4sLNtv6/W8u1T3JuBjpb0CeK7X856Bun8ROAd4cor9q4D7gQBWAg9P9z37ZWVQ5eMsVgNbS/srwAUREV2cYye0rTszd2bmy2VzF83f4Zjrqn58yY3ALcCPuzm5DqlS80eB2zPzMEBmHuzyHDuhSt0JnFrabwb+o4vz64jMfAg4dJwhq4E7s2kXcFpEnDGd9+yXMFgM7GvZ3l/6Jh2TmePAEeAtXZld51Spu9U6mj9NzHVt6y7L5qWZ2S+fZFbl7/qdwDsj4p8jYlf5JOC5rkrdfwJ8OCL2A9uA3+rO1HrqRP/vtzUrfs9AnRcRHwYGgV/q9Vw6LSJeB3wKWNvjqXTbfJqXioZorgAfioizM/PFXk6qC64EtmTmX0TE+4AvRsRZmfnfvZ7YXNIvK4MqH2dxbExEzKe5nHyhK7PrnEof4xERvwL8EXBJZr7Spbl1Uru6TwHOAhoR8RzNa6ojc/wmcpW/6/3ASGb+V2Z+G/h3muEwl1Wpex1wD0Bm/gvwRpof5tbPZvwjfPolDKp8nMUIsKa0LwMezHInZg5rW3dEvBf4PM0g6IdryNCm7sw8kpmLMnNZZi6jea/kksx8tDfTnRFV/o3/Pc1VARGxiOZlo71dnGMnVKn7u8AFABHx8zTD4D+7OsvuGwGuKk8VrQSOZObz0zlhX1wmyik+ziIibgAezcwR4A6ay8dRmjdmrujdjGdGxbr/HHgT8Dflfvl3M/OSnk16BlSsu69UrHk7cGFEPAW8CvxeZs7p1W/FujcAfxURv0PzZvLauf6DXkR8mWawLyr3Qq4HXg+QmZ+jeW9kFTAKvAxcPe33nON/ZpKkGdAvl4kkSdNgGEiSDANJkmEgScIwkCRhGEiSMAwkScD/ABrgAffQGkf3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_avg = test_df[['id', 'target']].copy()\n",
    "test_avg['target'] = 0\n",
    "# print(test_avg.describe())\n",
    "\n",
    "total_weight = 0\n",
    "for fold in Config.train_folds:\n",
    "#     test_weight = {key+f'_Fold_{fold}':value for key,value in oof_weight.items()}\n",
    "    test_weight = oof_weight #defaultdict(lambda:1)\n",
    "    test_df2 = pd.read_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\")\n",
    "#     print(test_df2.describe())\n",
    "    for col in test_df2.columns:\n",
    "        col_weight = col.split('_Fold_')[0]\n",
    "        if ('tta_' in col or 'preds' in col): \n",
    "#             print(col)\n",
    "#             print(test_weight[col_weight])\n",
    "            total_weight+=test_weight[col_weight]\n",
    "            test_avg['target'] += test_df2[col]*test_weight[col_weight]\n",
    "test_avg['target'] /= total_weight\n",
    "print(test_avg.describe())\n",
    "print(test_avg[\"target\"].hist(bins=100))\n",
    "print(test_avg)\n",
    "# print(total_weight)\n",
    "test_avg.to_csv(Config.model_output_folder + \"/test_avg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd406d-d7fd-4958-8e73-9d2caa624c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d50a2c45-3493-49b5-a750-d4bb4b33303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg[['id', 'target']].to_csv(\"./submission.csv\", index=False)\n",
    "\n",
    "test_avg[['id', 'target']].to_csv(Config.model_output_folder + \"/submission.csv\", index=False)\n",
    "\n",
    "!mkdir -p ~/.kaggle/ && cp $Config.kaggle_json_path ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd2f74d3-2783-4014-9b37-a0d6997797ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.01M/6.01M [00:07<00:00, 814kB/s]\n",
      "Successfully submitted to G2Net Gravitational Wave Detection"
     ]
    }
   ],
   "source": [
    "# !kaggle competitions submit -c g2net-gravitational-wave-detection -f ./submission.csv -m $Config.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6966b-8273-46fa-bc42-ec36bf4b2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarviscloud import jarviscloud\n",
    "jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea4e9c-95c5-4aba-9930-e13842e2c583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "340px",
    "width": "209.2px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
